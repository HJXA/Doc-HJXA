# 摘要
大型语言模型（LLM）在推理任务中已取得显著进展，但监督微调（SFT）与强化学习（RL）的最优融合仍是一项核心挑战。通过从基于熵的视角对令牌分布、学习动态及融合机制进行全面分析，我们揭示了这两种范式之间的关键差异：SFT会对LLM的策略分布产生粗粒度全局变化，而RL则会进行细粒度选择性优化，其中熵是衡量训练效果的关键指标。基于这些发现，我们提出了监督强化微调（SRFT）——一种通过熵感知加权机制将两种微调范式统一起来的单阶段方法。该方法不采用两阶段序贯方式，而是同时应用SFT与RL，利用演示数据和自探索轨迹直接对LLM进行优化。大量实验表明，SRFT的平均准确率达到59.1%，在5个数学推理基准上比无RL方法高出9.0%，在3个分布外基准上比无RL方法高出10.9%。

# 1 引言
近年来，大型语言模型（LLM）在推理任务领域取得了显著进展（OpenAI, 2025；Guo等, 2025；Anthropic, 2025），其在复杂问题解决任务中展现出卓越能力。尽管已取得这些亮眼成果，但用于提升推理能力的微调策略仍是当前研究的活跃方向，既蕴藏机遇，也面临挑战。

早期方法通常将监督微调（SFT）和强化学习（RL）视为相互独立的序贯阶段。例如，可先通过SFT实现模型对指令的遵循，再通过RL实现模型对齐。然而，这种分离模式存在诸多问题：SFT可能导致模型仅记忆数据模式，而无法形成真正的推理能力，进而可能对训练数据集产生过拟合（Chu等, 2025；Chen等, 2025a）。相反，RL方法虽在探索和奖励优化方面颇具潜力，但存在样本效率低、难以在庞大解空间中有效探索（Gao等, 2025；Dou等, 2025；Schmied等, 2025）等问题，还可能出现“模式崩溃”——即模型反复生成相似的次优输出（Cai等, 2025）。

不同于简单的序贯方法，近年来的研究工作（Yan等, 2025；Wu等, 2025；Liu等, 2025a；Chen等, 2025b；Liu等, 2025b）已呈现出向整合框架发展的趋势：这类框架或将SFT与RL范式统一，或在LLM训练过程中动态切换两种微调方法。如图1（a）所示，SFT引导LLM策略向演示数据分布靠拢，而RL则使策略能在基础策略的邻域内探索更优解。我们的示意图展示了一种特殊情况：当基础策略处于次优策略附近时，仅依靠RL轨迹无法有效导向最优策略。除了单独应用SFT和RL，将二者整合到单阶段方法（如本文提出的SRFT）中，能使策略在更广阔的空间内直接向更优解优化。然而，如何平衡SFT的知识蒸馏与RL的策略优化仍是一大挑战：整合不足可能导致误差传播，限制RL的提升空间；而过度依赖演示数据则会引发过拟合，制约策略在基础策略分布之外的探索。这种权衡关系让研究者在选择“SFT（利用演示数据）”还是“RL（进行策略探索）”时面临困惑。

为解决这些问题，本研究致力于构建单阶段LLM微调算法：该算法不仅能有效利用SFT数据集提升LLM推理能力，还能通过RL轨迹实现持续优化。我们对SFT和RL在LLM推理微调中的作用展开了全面分析，通过第3节的分析得出以下关键发现，为后续算法设计提供指导。

## 关键发现
- **策略分布影响（3.1.1节与3.1.2节）**：微调过程中，SFT对LLM的策略分布产生粗粒度全局改变，而RL则进行细粒度选择性调整。
- **单阶段优化（3.1.2节与3.2.2节）**：SFT与RL的单阶段整合能直接优化推理能力，且相比“先SFT后RL”的序贯方法，训练效率更高。
- **熵作为指标（3.2.1节）**：熵动态可揭示训练过程的内在机制，为两种范式的平衡加权提供依据。

基于这些见解，我们提出了“监督强化微调（SRFT）”——一种用于LLM推理的单阶段方法。如图1（b）所示，我们将SFT融入RL框架，并以熵为指标来控制两种范式的平衡。具体而言，对于LLM策略轨迹生成的样本，我们会根据样本奖励的正负采用不同的RL训练损失；对于来自演示数据集的样本，则同时应用SFT和RL目标函数。这种统一方法能实现多粒度下基于演示数据的稳定学习，同时有效融合SFT与RL的互补优势。

我们在5个竞赛级数学推理基准和3个分布外（OOD）基准上对该方法进行了评估。基于Qwen-2.5-Math-7B（Yang等, 2024）模型，本文提出的SRFT准确率达到59.1%，显著优于以往的SFT和RL基准模型。此外，SRFT还展现出卓越的泛化能力：与其他利用演示数据的方法相比，平均性能提升超过4.7%。

综上，本研究的核心贡献如下：
1. 对LLM推理中的SFT和RL进行了全面分析，考察了二者对策略分布和学习动态的差异化影响，并从基于熵的视角分析了SFT与RL的整合机制。
2. 提出了SRFT——一种单阶段微调方法，该方法通过熵感知加权机制融合监督微调与强化学习，能在有效利用演示数据的同时，维持稳定的探索动态。
3. 在8个具有挑战性的基准上验证了SRFT的卓越性能：在数学推理任务和分布外任务上，其性能分别比无RL基准模型提升了9.0%和10.9%。


# 2 预备知识
## 2.1 LLM推理中的SFT与RL
监督微调（SFT）是一种标准方法，用于将预训练语言模型适配到特定下游任务，或赋予模型特定的风格特征。给定数据集\(D=\{(x_i, y_i)\}\)（其中\(x_i\)为输入提示，\(y_i\)为行为策略\(\pi_\beta\)生成的对应目标响应），SFT的目标是训练语言模型策略\(\pi_\theta\)（参数为\(\theta\)），以最大化给定\(x_i\)时生成目标响应\(y_i\)的条件概率。这一目标通常通过最小化数据集上的负对数似然实现：
$$
\mathcal{L}_{SFT}(\theta)=\mathbb{E}_{(x, y) \in \mathcal{D}}\left[-\log \pi_{\theta}(y | x)\right] \tag{1}
$$
其中，\(y_j\)表示响应\(y\)中的第\(j\)个令牌，\(y_{<j}\)表示响应\(y\)中位于\(y_j\)之前的令牌序列。

强化学习（RL）通常在SFT之后应用，用于进一步使LLM对齐复杂的人类偏好或期望行为（如推理能力、无害性）——这类偏好或行为难以通过静态数据集详尽定义。在RL训练中，LLM的令牌生成过程被建模为马尔可夫决策过程（MDP）（Puterman, 2014）。我们将第\(t\)步的状态\(s_t\)定义为“输入提示\(x\)与截至当前生成的所有令牌”的拼接，该状态将作为策略模型的输入。具体而言，策略处理的输入为\(s_t = [x_1, x_2, ..., x_{|x|}, y_1, y_2, ..., y_{t-1}]\)（其中\(x_i\)表示输入\(x\)的第\(i\)个令牌，\(y_j\)表示\(\pi_\theta\)在第\(j\)步生成的令牌）。动作\(a_t\)对应下一个输出令牌\(y_t\)的选择。LLM作为策略\(\pi_\theta(a_t | s_t)\)，会针对提示\(x\)生成一个令牌序列轨迹\(y\)作为响应。奖励函数会根据提示\(x\)为整个轨迹\(y\)赋予一个标量分数，该分数通常来源于人类评估或自动指标。

在RL场景中，行为策略\(\pi_\beta(y | x)\)指生成回放缓冲区中响应的模型。该策略对RL至关重要（尤其在离策略学习中），因为它能通过适当的重要性采样修正，解决数据生成模型与当前训练模型之间的分布偏移问题。LLM中的MDP建模具有以下几个显著特征：
- **序贯状态表示**：在每一步\(t\)，状态由“输入提示\(x\)与截至当前生成的所有动作（令牌）\(y_{<t}\)”拼接而成，该状态将作为策略模型\(\pi_\theta(\cdot | s_t)\)的输入。
- **稀疏延迟奖励**：奖励\(R(x, y)\)通常具有稀疏性，仅在序列\(y\)生成完成后才会给出。这种对最终输出整体质量的依赖，使得生成过程中的信用分配问题更为复杂。

## 2.2 强化学习中的策略优化
为优化LLM策略，组相对策略优化（GRPO）（Shao等, 2024）提供了一种不同于现有方法的RL算法，它是近端策略优化（PPO）（Schulman等, 2017）的内存高效变体。GRPO的一个关键特征是，它通常无需学习价值函数；相反，对于给定提示\(x\)，GRPO会利用当前策略生成\(G\)个响应，并基于这些响应的奖励计算每个响应的相对优势：
$$
\hat{A}_{k}=\frac{R\left(x, y_{k}\right)-\text{mean}\left(\left\{R\left(x, y_{k}\right) \mid k=1,2, \ldots, G\right\}\right)}{\text{std}\left(\left\{R\left(x, y_{k}\right) \mid k=1,2, \ldots, G\right\}\right)} \tag{2}
$$

随后，GRPO通过最大化裁剪代理目标函数来实现稳定更新。设\(\pi_{\theta_{\text{old}}}\)为更新前的策略，对于轨迹\(y_k\)（来自状态\(s_t\)）中的每个令牌\(y_{k,t}\)，重要性采样比为\(r_{k,t}(\theta)=\frac{\pi_\theta(y_{k,t} | s_t)}{\pi_{\theta_{\text{old}}}(y_{k,t} | s_t)}\)。GRPO的目标函数可表示为：
$$
\mathcal{J}_{\text{GRPO}}(\theta)=\frac{1}{G} \sum_{k=1}^{G} \frac{1}{\left|y_{k}\right|} \sum_{t=1}^{\left|y_{k}\right|}\left[\min \left\{r_{k, t}(\theta) \cdot \hat{A}_{k}, \text{clip}\left\{r_{k, t}(\theta), 1-\epsilon, 1+\epsilon\right\} \cdot \hat{A}_{k}\right\}\right] \tag{3}
$$
其中，\(\epsilon\)是定义裁剪范围的小超参数。通过这一机制，LLM策略能在更新过程中维持稳定的梯度约束。

# 3 LLM推理中SFT与RL的分析
在本节中，我们将全面分析监督微调（SFT）和强化学习（RL）在LLM推理中的作用。首先，我们考察二者对令牌分布（3.1.1节）和学习动态（3.1.2节）的差异化影响，随后从基于熵的视角探究它们的整合机制（3.2节）。所有实验均在5个数学推理基准数据集（AIME24、AMC、MATH500、Minerva和Olympiad）上进行，结果取平均值。我们对所有基准方法的超参数进行了调优，以确保性能比较的公平性和最优性。

## 3.1 SFT与RL对LLM的影响：大锤式vs.手术刀式
### 3.1.1 对令牌分布的影响
为理解SFT与RL在推理任务中的差异化作用，我们以同一基础模型（Qwen-2.5-Math-7B）为研究对象，可视化了微调前后模型对相同提示生成响应时的令牌概率变化。如图2（a）所示，结果揭示了二者的本质差异：SFT会显著改变整个响应序列的概率分布，而RL仅对一小部分令牌的概率进行选择性调整，对数值内容和数学证明表述则基本不做改动。

我们进一步在5个基准数据集上量化了这种分布偏移，结果如图2（b）所示。数据表明，与RL相比，SFT对策略分布的改变更为显著——RL中令牌概率的变化值多集中在0附近，而SFT中令牌概率的变化幅度则明显更大。从理论角度来看，这种现象可通过SFT目标函数的梯度来解释：
$$
\nabla_{\theta} \mathcal{L}_{SFT}=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\sum_{t=1}^{|y|} \sum_{v \in \mathcal{V}}\left(\pi_{\theta}\left(v | x, y_{<t}\right)-1_{v=y_{t}}\right) \nabla_{\theta} \log \pi_{\theta}\left(v | x, y_{<t}\right)\right]
$$
其中，\(v\)代表LLM的词汇表，\(1_{v=y_{t}}\)为指示函数——当令牌\(v\)与目标令牌\(y_t\)匹配时，该函数值为1，否则为0（详细推导过程见附录D）。这一公式表明，SFT会系统性地锐化模型分布：通过提高目标令牌的概率，同时降低词汇表中所有其他令牌的概率，最终使模型输出更具确定性。

（图2：LLM微调过程中的分布变化可视化。（a）热图对比：微调后模型与基础模型生成的响应，背景颜色越深表示概率变化越大；（b）5个数学推理基准数据集上令牌概率变化的分布情况）

### 3.1.2 学习动态的可视化
除令牌概率分析外，我们还从学习动态的视角对两种训练范式进行了研究。由于直接测量LLM的特征空间计算成本极高，我们提出了一种新颖的可视化方法：将每个模型映射到词汇概率空间中的一个点——即把模型视为“将提示转换为词汇表输出概率分布”的函数。我们选取了三个参考模型——基础模型（Qwen-2.5-Math-7B）、DeepSeek-R1和QwQ-32B（Team, 2025）——作为坐标系，通过概率空间中的距离（若两个模型对所有提示下的所有令牌均分配相似的输出概率，则认为二者接近），间接衡量模型在不同微调步骤中的演化情况（该可视化方法的详细流程见附录E）。

图3展示了可视化结果，从中可观察到：所有微调范式在提升模型性能的同时，均会使模型远离基础模型（Qwen-2.5-Math-7B）所在的空间。具体而言，与RL相比，SFT使模型分布偏离基础模型的程度更大，且能实现更高的性能——这一结果进一步验证了我们在3.1.1节中的发现，即SFT会对模型分布产生更大幅度的改变，而RL微调则仅在初始模型附近的邻域内进行。

我们还分析了两种整合方式：两阶段“先SFT后RL”方法，以及下文将详细介绍的单阶段SRFT方法。结果显示，两阶段“先SFT后RL”方法的学习动态表现为：模型从SFT后的状态向性能更高的区域迁移，但该区域反而更接近基础模型——这表明初始的SFT阶段可能导致模型分布偏离基础模型过多，从而影响后续RL的效果。相比之下，我们提出的单阶段方法在概率空间中的变化更具约束性且针对性更强，与序贯式整合方法相比，能实现更精准的优化。

（图3：不同微调范式在三维概率空间中的学习动态。数字代表各训练过程的最终性能）

## 3.2 SFT与RL的整合：从两阶段到单阶段
### 3.2.1 序贯式整合分析
在本节中，我们从熵动态的视角考察SFT与RL的整合，以理解二者在LLM微调中的互补作用。首先，我们系统分析了两种序贯式整合方法：“先SFT后RL”和“先RL后SFT”，结果如图4所示。

如表1和图4（a）所示，在RL之后进行SFT的“先RL后SFT”方法，在所有基准数据集上的性能均始终处于次优水平。为缓解“先RL后SFT”方法导致的不利策略偏移，我们引入了KL散度约束（即\(SFT_{KL}\)，详细内容见附录B）来正则化分布变化。然而，即便加入该约束，性能提升仍十分有限——这表明“先RL后SFT”的顺序存在本质上的不兼容性。

相比之下，现有研究表明，在基础模型经过SFT后再应用RL的“先SFT后RL”方法，能成功实现显著的性能提升（如表1所示）。这种不对称现象表明，微调范式的顺序会对模型最终性能产生关键影响，这也促使我们通过基于熵的分析来理解其背后的机制。

为解释这种不对称性，我们从熵的视角分析了SFT和RL的训练动态。如图4（b）所示，经过RL训练后的策略熵值显著降低，模型输出趋近于确定性。然而，后续SFT带来的分布偏移会导致熵值快速上升（与图4（a）中性能的急剧下降相对应），随后又逐渐下降。此外，经过RL训练的模型，其通过SFT进一步学习的能力十分有限——这一点可从图4（b）中观察到：约90个训练步骤后，模型熵值进入平台期。

与之相反，基础模型在SFT过程中，熵值会先经历短暂上升，随后持续下降，最终实现性能提升。这种截然不同的熵轨迹表明：尽管RL能有效提升LLM性能，但同时也会降低模型的可塑性（即通过后续训练实现自适应的能力）。这些发现证明，熵是实现SFT与RL有效整合的关键指标。

（表1：不同SFT与RL整合策略在多个基准数据集上的性能比较。粗体和下划线分别表示最优和次优性能）

| 模型                | AIME24 | AMC  | MATH500 | Minerva | Olympiad | 平均值  |
|---------------------|--------|------|---------|---------|----------|---------|
| Qwen2.5-Math-7B     | 14.1   | 44.8 | 64.8    | 16.5    | 29.6     | 34.0    |
| SFT                 | 21.2   | 53.2 | 83.0    | 37.1    | 42.2     | 47.3    |
| RL                  | 21.2   | 59.3 | 83.6    | 36.4    | 46.6     | 49.4    |
| 先RL后SFT           | 10.5   | 40.4 | 73.6    | 32.0    | 30.7     | 37.4    |
| 先RL后SFT（含KL约束）| 13.1   | 45.2 | 70.2    | 26.5    | 36.3     | 38.3    |
| 先SFT后RL           | 24.5   | 59.3 | 86.4    | 39.3    | 53.1     | 52.5    |

（图4：“先SFT后RL”与“先RL后SFT”方法的比较。（a）两种方法推理能力的对比；（b）两种微调范式的熵动态）

### 3.2.2 单阶段整合分析
基于上述分析，我们确定“先SFT后RL”范式比“先RL后SFT”更适合LLM推理任务。但除了这些序贯式整合方法外，我们还研究了一种将两种范式直接统一的单阶段方法（即SFT+RL），其组合目标函数为\(\mathcal{L}_{combined} = \mathcal{L}_{SFT} + \mathcal{L}_{RL}\)。

我们进行了一项初步实验，对比了纯RL、不同SFT步骤的序贯式“先SFT后RL”，以及单阶段SFT+RL方法，结果如图5所示。实验发现，单阶段SFT+RL方法的训练效率优于序贯式“先SFT后RL”方法。值得注意的是，我们观察到一个有趣现象：对于经过大量SFT预训练的模型（先进行350步SFT，再进行150步RL），在RL训练初期会出现短暂的性能下降。我们将这种现象归因于两个主要因素：

1. 即便源自高质量演示数据，SFT数据集（由其他模型生成的响应构成）也未必能始终代表最优解，这可能导致SFT阶段学到次优策略；
2. 纯RL的数据效率较低，因其无法有效利用演示数据。在序贯式“先SFT后RL”训练范式中，RL阶段可能会导致模型对SFT阶段习得知识的“灾难性遗忘”（Cai等, 2025），进而引发短暂的性能下降。

相比之下，单阶段SFT+RL方法通过统一优化，能有效利用演示数据。该方法可直接朝着目标任务优化策略，同时保留监督学习从数据集中蒸馏知识的优势。重要的是，两种利用数据集的方法（单阶段SFT+RL和序贯式“先SFT后RL”）在所有性能指标上均显著优于纯RL。

（图5：纯RL、序贯式“先SFT后RL”与单阶段SFT+RL整合方法的初步对比）

# 4 方法
在本节中，我们将介绍监督强化微调（Supervised and Reinforcement Fine-tuning, SRFT）算法。该算法通过单阶段方法整合监督微调（SFT）与强化学习（RL）的优势，在第2.2节所述RL框架的基础上，融入来自演示数据的灵活引导，使算法能够充分利用两种微调范式的互补优势。SRFT的核心创新在于其单阶段学习机制：通过SFT实现对行为策略的粗粒度逼近，通过RL实现对策略的细粒度优化，且这两种操作会同时应用于演示数据和模型自主生成的试错数据。

## 4.1 从演示数据中学习
给定包含演示数据的数据集\(D_{demo. }\)（例如由DeepSeek-R1生成的推理响应），SRFT采用双策略来高效利用这一宝贵资源。首先，我们借助SFT对专家响应背后的行为策略进行粗粒度逼近。行为策略\(\pi_{\beta}(y | x)\)捕捉了生成这些高质量响应的潜在生成模式，我们通过监督学习对其进行逼近，对应的损失函数为：
$$
\mathcal{L}_{SFT}^{demo. }=\mathbb{E}_{(x, y) \sim \mathcal{D}_{demo }}\left[-\log \pi_{\theta}(y | x)\right] \tag{5}
$$

其次，我们采用与LUFFY（Yan et al., 2025）类似的离策略RL方法，通过RL对行为策略进行细粒度学习。具体而言，我们将演示数据直接加入LLM的在策略轨迹组中，构建一个异质训练批次：
$$
G_{aug. }=\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{\left|G_{roll }\right|} \cup\left\{\left(x_{j}, y_{j}\right)\right\}_{j=1}^{\left|G_{demo }\right|} \tag{6}
$$
其中，\(G_{roll. }\)表示在策略轨迹组，\(G_{demo. }\)表示演示数据组。整个增强训练组的优势估计公式为：
$$
\hat{A}_{k}=\frac{r\left(x, y_{k}\right)-\text{mean}\left(\left\{r\left(x, y_{k}\right)|k=1,2, \ldots,\left|G_{aug. }\right|\right\}\right)}{\text{std}\left(\left\{r\left(x, y_{k}\right) \mid k=1,2, \ldots,\left|G_{aug. }\right|\right\}\right)}
$$

由于专家LLM生成的响应通常具有更高的奖励，如公式（2）所示，将其纳入训练组会提高整个组的优势估计值，通过这一机制可促进LLM策略的乐观探索。

针对我们分析中发现的“演示数据行为策略\(\pi_{\beta}\)与当前训练策略\(\pi_{\theta}\)存在分布失配”问题，我们实施了两项关键缓解策略：
- **演示数据SFT的熵感知权重**：第3.2节的熵分析表明，熵是实现SFT与RL有效整合的关键指标。基于这一洞见，我们引入自适应权重机制，该机制根据当前策略的熵值动态调整权重，采用\(w_{SFT} = \exp(-\text{stop\_grad}(H(\pi_{\theta})))\)作为SFT权重（其中\(\text{stop\_grad}(\cdot)\)表示梯度停止操作）。这种熵感知机制可确保：当策略熵值较高（表明模型存在不确定性）时，SFT训练损失对模型更新的影响会减弱，从而在实现有效行为策略逼近的同时，缓解因演示数据行为策略与当前策略分布失配导致的性能退化。此时，SFT损失函数更新为：
  $$
  \mathcal{L}_{SFT}^{demo. }(\theta)=w_{SFT} \cdot \mathbb{E}_{(x, y) \sim \mathcal{D}_{demo. }}\left[-\log \pi_{\theta}(y | x)\right] \tag{8}
  $$

- **离策略RL训练的重要性采样**：我们引入与GRPO（Shao et al., 2024）和PPO（Schulman et al., 2017）类似的重要性采样项，以解决行为策略与当前策略之间的分布偏移问题。对应的RL损失函数为：
  $$
  \mathcal{L}_{RL}^{demo.}(\theta )=-\mathbb{E}_{(x,y)\sim \mathcal{D}_{demo}}\left[ \min\left\{ r_{k,t}(\theta )\cdot \hat{A}_{k},\text{clip}\left\{ r_{k,t}(\theta ),1-\epsilon ,1+\epsilon \right\} \cdot \hat{A}_{k}\right\} \right] \tag{9}
  $$
  其中，重要性采样比定义为：
  $$
  r_{k, t}(\theta)=\frac{\pi_{\theta}\left(y_{k, t} | x_{t}\right)}{\pi_{\beta}\left(y_{k, t} | x_{t}\right)} \tag{10}
  $$

参考近期研究成果（Yan et al., 2025; Ma et al., 2025）的实践，我们将行为策略\(\pi_{\beta}\)设为1，以避免在对齐当前训练策略与行为策略时出现的令牌化复杂性，从而无需重新计算行为策略概率即可轻松整合现成数据集。此外，由于当\(\pi_{\beta}=1\)时，标准裁剪机制会出现失衡且可能不稳定，因此我们省略了裁剪操作。

## 4.2 从自探索中学习
除利用演示数据外，SRFT还能使LLM策略同时从自身的探索轨迹中学习。传统RL方法会从轨迹生成过程中的正负样本中学习，而我们发现：在采用二元奖励{1, −1}的在策略RL中，基础RL目标函数可自然分解为两个不同组件：
$$
\begin{aligned} 
\mathcal{L}_{RL}^{self-rollout } & =-\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(\cdot | x)}\left[R(x, y) \log \pi_{\theta}(y | x)\right] \\ 
& =\underbrace{\mathbb{E}_{x \sim \mathcal{D}, y^{+} \sim \pi_{\theta}(\cdot | x)}\left[-\log \pi_{\theta}\left(y^{+} | x\right)\right]}_{\text{正样本（1）}}+\underbrace{\mathbb{E}_{x \sim \mathcal{D}, y^{-} \sim \pi_{\theta}(\cdot | x)}\left[\log \pi_{\theta}\left(y^{-} | x\right)\right]}_{\text{负样本（2）}} \quad(11)
\end{aligned}
$$
其中，\(\mathcal{D}\)表示RL训练数据集，\(y^{+}\)和\(y^{-}\)分别代表正确响应和错误响应。从这一分解中可得出一个关键洞见：正样本目标（1）在结构上与监督微调相似，均旨在最大化正确响应的似然；但不同的是，这些正样本是由当前策略\(\pi_{\theta}\)在策略内生成的，而非来源于SFT数据集，这一特点使我们的方法有别于传统监督学习范式。负奖励组件（2）则实现似然最小化，系统性地降低模型对错误响应的概率分配。这种结构对应关系表明，从正样本中学习属于粗粒度优化策略，需要谨慎平衡。

此外，与从演示数据中学习不同，自探索会导致模型熵值快速下降（模型收敛于确定性更高的输出），这可能会损害模型的探索能力。为缓解这一现象并维持训练稳定性，受第3.1.1节分析的启发，我们针对正样本目标引入熵自适应权重机制。该机制与公式（8）中的形式类似，但作用互补——旨在维持探索多样性。完整的自探索目标函数可表示为：
$$
\mathcal{L}_{RL}^{self-rollout }(\theta)=w_{RL} \cdot \mathbb{E}_{x \sim \mathcal{D}, y^{+} \sim \pi_{\theta}(\cdot | x)}\left[-\log \pi_{\theta}\left(y^{+} | x\right)\right]+\mathbb{E}_{x \sim \mathcal{D}, y^{-} \sim \pi_{\theta}(\cdot | x)}\left[\log \pi_{\theta}\left(y^{-} | x\right)\right] \tag{12}
$$

## 4.3 单阶段方法中整合演示数据与自探索轨迹
通过同时利用演示数据和模型自主生成的轨迹，SRFT在整个单阶段微调过程中，有效平衡了SFT的粗粒度调整与RL的细粒度优化。总损失函数整合了上述四个组件：
$$
\mathcal{L}_{SRFT }(\theta)=\mathcal{L}_{SFT}^{demo. }(\theta)+\mathcal{L}_{RL}^{demo }(\theta)+\mathcal{L}_{RL}^{self-rollout }(\theta) \tag{13}
$$

该目标函数使SRFT能够同时从演示数据和自探索轨迹中获益，同时通过两种熵感知权重机制维持稳定的训练动态。

# 5 实验
## 5.1 实验设置
### 训练数据集
我们采用OpenR1-Math-46k-8192¹（Yan等，2025）作为SRFT的训练数据集。该数据集是OpenR1-Math-220k（Face，2025）的子集，包含4.6万个数学问题（源自NuminaMath 1.5（Li等，2024）），并附带由DeepSeek-R1（Guo等，2025）生成的高质量推理响应。我们通过Math-Verify²对数据集进行过滤，剔除答案无法验证或响应长度超过8192个令牌的样本。该数据集在我们的框架中承担多重角色：为策略轨迹生成提供提示、为奖励计算提供真值答案，以及为SRFT提供高质量演示数据。数据集详细信息见附录C。

¹https://huggingface.co/datasets/Elliott/Openr1-Math-46k-8192  
²https://github.com/huggingface/Math-Verify

### 评估方式
我们在多个广泛使用的数学推理基准上进行全面评估，包括AIME24（Li等，2024）、AMC（Li等，2024）、Minerva（Lewkowycz等，2022）、OlympiadBench（He等，2024）和MATH500（Hendrycks等，2021）。对于样本量较小的数据集（AIME24和AMC），我们采用avg@32指标；对于其余三个数据集，则使用pass@1作为评估标准。由于我们的方法主要聚焦于数学推理能力，我们进一步在三个分布外（OOD）基准上评估模型的泛化能力：ARC-C（Clark等，2018）（开放域推理）、GPQA-Diamond（Rein等，2024）（研究生阶段科学知识，简称GPQA-D）和MMLU-Pro（Wang等，2024）（源自学术考试和教材的推理题）。为降低信息泄露风险，我们对所有选择题的选项顺序进行随机打乱。推理过程中，我们将生成温度设为0.6，最大响应长度设为8192个令牌。训练验证采用Math-Verify作为验证工具，最终评估则使用OAT-Grader（Liu等，2024）。

### 基线方法
我们以Qwen2.5-Math-7B为基础模型，将SRFT与以下基线方法进行对比：
- **SFT方法**：（1）在OpenR1-Math-46k-8192数据集上进行SFT；（2）在损失函数中加入KL散度约束的SFT（SFT-KL）。
- **RL方法**：（3）RL-GRPO（Shao等，2024）——在相同4.6万数据集上训练的简化PPO变体；（4）SimpleRL-Zero（Zeng等，2025）——将GRPO应用于约2.4万个来自GSM8K和MATH的数学样本；（5）OpenReasoner-Zero（Hu等，2025）——基于PPO的方法，在包含AIME等多源数据的12.9万样本上训练；（6）PRIME-Zero（Cui等，2025）——在15万个NuminaMath查询上进行策略轨迹生成，采用隐式过程奖励和最终标签。
- **SFT与RL结合的方法**：（7）SFT→RL——在相同4.6万数据集上先进行SFT，再进行GRPO训练的序贯方法；（8）ReLIFT（Ma等，2025）——在最难问题上交替进行RL和在线微调的方法；（9）LUFFY（Yan等，2025）——使用相同4.6万数据集的混合策略GRPO方法；（10）TAPO（Wu等，2025）——在GRPO框架内动态整合结构化外部知识的方法，在MATH数据集的5.5千个样本上训练。

### 实现细节
参考近期研究（Yan等，2025；Wu等，2025；Cui等，2025）的做法，我们以Qwen2.5-Math-7B（Yang等，2024）作为基础模型。在SRFT中，每个提示生成8条轨迹，最大序列长度为8192个令牌。所有实验均进行500个训练步骤。详细实验细节见附录B。

## 5.2 实验结果
### 推理基准性能
主要结果如表2所示，我们将SRFT与多个无RL基线、纯SFT方法以及SFT+RL方法进行了对比。在5个具有挑战性的竞赛级推理基准上，SRFT的平均得分为59.1，显著优于现有RL方法——比最佳基线高出9.0分，充分证明了在LLM推理中融合演示数据与自探索的优势。我们还观察到，SRFT比SFT方法高出4.8分，这表明自探索组件能有效优化从演示数据中学习到的策略分布。与SFT+RL方法相比，SRFT高出3.4分，这说明单阶段设计和熵感知加权机制能有效平衡演示数据与自探索的优势。

### 分布外泛化能力
关于分布外性能，表2结果显示，SRFT的平均得分为62.5，比最佳基线高出4.7分。这些结果凸显了SRFT通过融合演示数据与自探索提升泛化能力的有效性。

### 训练动态
图6展示了SRFT的训练动态，包括训练奖励、响应长度和训练熵。如图6（a）所示，与RL相比，SRFT实现了更快的性能提升，且两者的训练奖励均呈上升趋势。在响应长度方面（图6（b）），面对具有挑战性的训练数据时，RL倾向于生成更简洁的响应，而SRFT的响应长度则逐步增加，这表明模型形成了更全面、详细的推理过程。从熵的角度（图6（c）），与RL的熵值快速下降不同，我们的SRFT方法能维持更稳定的熵值，这意味着策略在训练过程中仍能持续探索——这也验证了熵感知加权机制的有效性。

（图6：RL与SRFT训练过程中的训练动态，包括训练奖励、响应长度和训练熵）

### 消融实验
我们通过消融实验评估各组件的有效性。如表3所示，我们重点分析了两个关键熵感知加权机制的影响：用于演示数据学习的\(w_{SFT}\)和用于自探索正样本学习的\(w_{RL}\)。我们通过将这些权重设为固定值1.0来剔除相应机制，以评估这两个加权组件的作用。剔除SFT加权机制（w/o \(w_{SFT}\)）会导致性能下降4.0分，而剔除RL加权机制（w/o \(w_{RL}\)）会导致性能下降2.9分——这表明两个组件均对整体性能有显著贡献。消融结果验证了我们的理论分析：熵感知加权机制使SRFT能够动态平衡监督学习与强化学习组件，相比固定加权方案，实现了更稳定的训练和更优的性能。

（表2：基于Qwen2.5-Math-7B在5个竞赛级数学推理基准和3个分布外基准上的整体性能。粗体和下划线分别表示最佳和次优性能）

（表3：SRFT的消融实验结果，包括\(w_{SFT}\)和\(w_{RL}\)的影响）

| 模型                | AIME24 | AMC  | MATH500 | Minerva | Olympiad | 平均值  |
|---------------------|--------|------|---------|---------|----------|---------|
| Qwen2.5-Math        | 11.4   | 32.6 | 48.8    | 8.7     | 15.8     | 23.5    |
| SRFT（无\(w_{SFT}\)）| 30.1   | 65.8 | 87.0    | 36.8    | 55.8     | 55.1    |
| SRFT（无\(w_{RL}\)） | 32.6   | 67.2 | 87.5    | 37.4    | 56.5     | 56.2    |
| SRFT（完整版本）    | 35.3   | 72.2 | 89.8    | 39.7    | 58.3     | 59.1    |


# 6 相关工作
## 6.1 LLM推理的强化学习
在LLM复杂推理能力的研究中，强化学习（RL）已成为突破监督微调（SFT）局限的关键方法，并取得了显著进展。近年来，GRPO（Shao等，2024；Guo等，2025）、DAPO（Yu等，2025）、DR.GRPO（Liu等，2025c）和VAPO（Yue等，2025a）等方法在数学推理和复杂问题解决任务中展现出优异性能。然而，RL提升推理能力的具体机制仍未完全明确。多项实证研究表明，强化学习的核心作用可能是“激发、优化或改进对模型已有推理能力的采样”，而非从零开始灌输全新的基础推理技能。例如，Yue等（2025b）提出疑问：当前基于可验证奖励的强化学习（RLVR）是否真的能扩展推理边界（提升pass@k），还是仅能提高对已有解决方案的采样效率（提升pass@1）。类似地，Wang等（2025a）指出，基础模型本身已具备较强的推理能力，强化学习的作用在于有效“激活”或“引导”这些能力。尽管如此，ProRL（Liu等，2025d）的研究表明，经过持续稳定的强化学习训练，模型在基础模型完全失效的任务上也能实现更高的成功率——这意味着RL确实能扩展LLM的推理能力边界。本研究设计的单阶段方法将SFT与RL融合，在训练过程中维持稳定的熵值，实现了性能的持续提升。

## 6.2 监督微调与强化学习的整合
SFT与RL的协同作用是现代LLM发展的核心研究方向之一。基于高质量推理链的SFT能为模型奠定坚实的初始策略基础，后续RL可在此基础上进一步优化。Cai等（2025）研究了SFT后模型所需的探索程度，发现RL通过让模型偏离可能次优的SFT轨迹，仍能带来显著性能提升。近期研究（Chen等，2025a）表明，SFT可为模型提供结构化推理模板，而RL则负责验证和改进这些模板。尽管如此，如何优化这两种互补范式的融合策略，仍是当前研究的热点问题。

为提升样本效率并为RL探索提供结构化引导，研究者们探索了多种将外部监督融入强化学习框架的方法：
- UFT（Liu等，2025a）提出了一种新范式，将SFT与RL统一为单一过程，利用部分解决方案的提示等信息性监督信号引导探索，加速收敛；
- LUFFY（Yan等，2025）针对在策略学习的局限性，通过引入更强模型的离策略推理轨迹来增强RLVR，动态平衡模仿学习与在策略探索，以提升模型能力；
- ReLIFT（Ma等，2025）为解决纯RL的局限，将强化学习与训练过程中收集的高质量演示数据的监督微调交替进行，使模型能获取超出初始能力范围的新知识；
- TAPO（Wu等，2025）通过从历史样本中提取“思维模式”作为外部高层引导，将其自适应整合到RL框架中，平衡模型的内部探索与外部策略利用；
- SASR（Chen等，2025b）提出了一种混合框架，从理论上统一SFT与RL：先通过SFT进行初始预热，再根据训练动态将SFT与在线RL自适应融合，在探索多样化路径的同时维持核心推理能力，并将高质量SFT演示数据作为关键外部数据源。

此外，SFT与RL的单阶段整合有助于缓解以往方法在从SFT过渡到RL时面临的“灾难性遗忘”问题（Chen等，2025b；Liu等，2025b）。这些方法共同体现了一个重要趋势：在强化学习框架中更复杂地整合监督信号，以提升推理对齐效果和整体性能。


# 7 结论
本研究围绕LLM推理任务中监督微调（SFT）与强化学习（RL）的整合展开探索。通过全面分析，我们发现SFT会对模型策略进行粗粒度全局调整，而RL则进行细粒度选择性优化，其中熵是衡量训练效果的关键指标。基于这些发现，我们提出了SRFT——一种通过熵感知加权机制将两种范式统一的单阶段方法。大量实验验证了SRFT的有效性：其平均准确率达59.1%，在推理任务上比无RL基线高出9.0%，在分布外基准上比无RL基线高出10.9%。

## 局限性
尽管我们的研究证明了基于熵感知的SFT-RL单阶段整合的有效性，但目前对熵动态的利用仍较为简单（仅采用基础指数加权函数）。训练过程中熵的丰富时间模式表明，未来可探索更复杂的基于熵的控制机制——例如自适应熵调度或多时间尺度熵分析，以更好地捕捉SFT与RL信号之间的相互作用，进而开发出更具理论依据的混合训练算法。此外，我们的方法假设可获取高质量演示数据，未来研究可探索在“不完美演示数据”上的训练策略，以提升该方法的适用性。

# 附录
## A 更多实验结果
### A.1 SRFT的令牌概率可视化
我们对训练后的SRFT模型的令牌概率分布进行了可视化，结果如图A1所示。可以观察到，SRFT模型的令牌概率变化幅度适中，在SFT与RL之间找到了平衡点——既提升了模型的推理能力，又保留了模型的基础能力。

（图A1：SRFT的令牌概率分布可视化。注：“微调后概率降低”“概率相近”“微调后概率升高”分别对应不同令牌的概率变化情况）

### A.2 熵感知梯度裁剪
通过对微调所修改令牌的熵特征进行研究，我们发现强化学习（RL）主要针对熵分布较高的令牌进行优化——这一发现与近期关于语言模型选择性优化的研究（Wang等，2025b）结论一致。为实证验证这一观察，我们设计了对照实验：在RL训练过程中，对高概率（低熵）令牌实施梯度截断。如图A2所示，即便对低熵令牌的梯度进行截断，模型性能仍与原始RL算法相近，这为我们的假设提供了强有力的实证支持。该结果表明，RL的优化具有显著的选择性——它会精准调整分布不确定（高熵）的令牌，而对模型已确定（低熵）的预测结果基本不做改动。与之相反，SFT会对整个令牌空间进行广泛修改，从根本上改变模型的分布特征，且这种修改的区分度较低。

（图A2：对低熵令牌实施梯度裁剪后的RL性能。注：“Prob_clip 0.5/0.8/0.9”分别表示对概率高于0.5、0.8、0.9的令牌进行梯度裁剪；“Base Policy”为基础策略）


## B 实验细节
### B.1 训练
我们遵循OpenR1-Qwen-7B（Face，2025）的SFT配置，对DeepSeek-R1生成的推理轨迹和提示进行全参数微调。训练超参数设置如下：批次大小（batch size）为128，学习率为\(2\times10^{-5}\)，采用线性学习率调度策略（10%预热比例），训练轮次（epoch）为3。

对于加入KL正则化的SFT（即\(SFT_{KL}\)），我们采用与上述相同的设置，同时在当前策略与基础模型（Qwen2.5-Math-7B）之间加入KL散度正则项，正则项权重设为\(0.1\)。\(SFT_{KL}\)的损失函数为：
$$
\mathcal{L}_{SFT_{KL}}(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[-\log \pi_{\theta}(y | x)\right]+\lambda \mathcal{L}_{KL}\left(\theta, \theta_{base }\right) \tag{A1}
$$
其中，\(\mathcal{L}_{KL}(\theta, \theta_{base })\)表示当前策略与基础模型之间的KL散度。

对于RL训练：共进行500个训练步骤，每个提示生成8条轨迹，学习率固定为\(1\times10^{-6}\)。

对于本文提出的SRFT方法：采用与RL相同的训练设置。由于Qwen2.5-Math-7B的最大序列长度为4096，无法满足本研究任务需求，我们将旋转位置编码（RoPE）的theta参数从10000调整为40000，并将窗口大小扩展至16384。

所有实验均使用verl框架（Sheng等，2024）实现¹，且在64张A100 GPU上进行。

¹https://github.com/volcengine/ver1

### B.2 评估
所有评估均使用VLLM框架（Kwon等，2023）进行，生成温度设为0.6，最大生成长度为8192个令牌。对于样本量较小的数据集（AIME24和AMC），采用avg@32指标；对于其余三个数据集（MATH500、Minerva、Olympiad），采用pass@1指标。我们使用Math-Verify和OAT-Grader（Liu等，2024）验证生成答案的正确性。

在基线方法对比中，我们独立验证了基础模型、SFT相关基线、GRPO（Shao等，2024）、LUFFY（Yan等，2025）和ReLIFT（Ma等，2025）的结果；而TAPO（Wu等，2025）及其他零样本强化学习模型的结果，则引自TAPO论文和LUFFY论文（因无法获取其开源代码或模型）。

### B.3 奖励设计
为评估我们方法的效果，我们采用如下简单奖励函数，所有训练实验均使用该奖励函数：
$$
R(x, y)=\begin{cases} 
1, & \text{若响应} \ y \ \text{正确} \\ 
0, & \text{否则} 
\end{cases}
$$

### B.4 对话模板
参考Yan等（2025）和Ma等（2025）的做法，所有训练范式（SFT、RL、SRFT）均采用统一的系统提示，以引导模型进行系统性推理，具体模板如图A3所示。此外，我们还测试了其他备选模板，如图A4所示。

#### （1）本文使用的对话模板
你的任务是在给出最终解决方案前，遵循系统、全面的推理流程。这包括通过多轮迭代进行分析、总结、探索、重新评估和完善思考过程。请将响应分为“思考（Thought）”和“解决方案（Solution）”两部分：
- 在“思考”部分，使用“<cthink>（思考内容）</cthink>\n”的格式呈现推理过程。每个思考内容需包含对思路的详细分析、头脑风暴、验证和完善。
- 在“解决方案”部分的“</cthink>\n”之后，基于“思考”部分的探索，给出最终、符合逻辑且准确的答案。
- 若适用，对于选择题或数学题等封闭式结果，请将答案用\(\boxed{}\)标注。

问题：{question}
答案：{answer}

（图A3：所有训练范式（SFT、RL、SRFT）使用的对话模板）

#### （2）Qwen模型的对话模板
请逐步推理，并将最终答案用\(\boxed{}\)标注。

问题：{question}
答案：{answer}

（图A4：Qwen-2.5-Math模型使用的对话模板）

### B.5 模板消融实验
为尽量降低模板对实验结果的影响，我们使用不同模板对基础模型Qwen-7B-Math进行了评估，结果如表A1所示。该结果表明，我们设计的模板能有效引导模型的推理过程，且在不同数学领域中保持一致性。

（表A1：不同模板在数学推理基准上的消融实验结果）

| 模板          | AIME24 | AMC  | MATH500 | Minerva | Olympiad | 平均值  |
|---------------|--------|------|---------|---------|----------|---------|
| 无模板        | 0.302  | 0.132 | 0.596   | 0.424   | 0.134    | 0.318   |
| Qwen2.5-Math模板 | 0.144  | 0.088 | 0.446   | 0.303   | 0.111    | 0.218   |
| 本文模板（SRFT） | 0.296  | 0.165 | 0.648   | 0.448   | 0.141    | 0.340   |


## C 数据集详情
### C.1 训练数据集
OpenR1-Math-220k是一个专为数学推理设计的综合数据集，包含22万个数学问题。每个问题都配有2-4条由DeepSeek-R1模型生成的推理轨迹。这些轨迹已通过Math-Verify和Llama3.3-70B-Instruct等工具验证，确保每个问题至少包含一条正确的推理路径。该数据集旨在挑战模型对复杂推理过程的理解与复现能力，涵盖代数、几何、数论、微积分等多个数学领域。

### C.2 评估基准
为评估上述模型，我们采用8个基准数据集，分为“数学推理基准”和“分布外（OOD）基准”两类，具体如下：

#### C.2.1 数学推理基准
- **AIME24**：基于2024年美国数学邀请赛（American Invitational Mathematics Examination, AIME）试题的基准数据集。该基准要求模型通过生成逐步解决方案来解决具有挑战性的数学问题，涵盖初等代数、几何、三角学、数论、概率与组合数学等主题。数据集以JSONL格式组织，每条数据对应一道完整试题；考试共15道题，答案为0-999之间的整数，需高级数学推理与解题技巧。
- **AMC**：包含美国数学竞赛（American Mathematics Competitions, AMC）试题的验证数据集，具体为2022年和2023年的AMC12试题，共83道题，均从AOPS维基页面提取。AMC10竞赛包含25道选择题，考试时长75分钟，面向10年级及以下学生，内容涵盖初等代数、基础几何（面积与体积公式）、初等数论和初等概率。该数据集作为内部验证集，聚焦于与AMC12和AIME难度相当的竞赛级数学问题。
- **MATH500**：精心筛选的数学问题子集，用于稳健评估。它是从Hendrycks于2021年发布的MATH数据集中随机抽取500道题组成的子集——2024年底，OpenAI因将原始MATH数据集（含5000道题）的90%用于o1系列模型的强化学习训练，故构建了该子集。MATH500在保留原始MATH基准多样性与复杂性的同时，提供了一个无数据污染风险的清洁评估集。
- **Minerva**：涵盖多个数学领域和难度级别的数学推理基准，旨在评估模型解决高级数学问题的能力，内容包括高中至大学阶段的数学知识。该基准包含需多步推理、符号运算和深度数学理解的问题，覆盖多个数学领域。
- **Olympiad（奥林匹克数学基准）**：包含奥林匹克级别数学问题的基准，代表最具挑战性的数学推理任务。与现有奥林匹克相关基准不同，该类数据集仅聚焦数学领域，包含大量竞赛级试题，并细分为33+个子领域、10+个难度级别。这些试题需出色的数学洞察力、创造力和高级解题技巧（常见于国际数学竞赛），本研究采用其中的OE_TO_MATHS_EN_COMP子集进行评估。

#### C.2.2 分布外（OOD）基准
- **ARC-C（AI2推理挑战-挑战级）**：包含小学阶段科学问题的数据集，需常识推理与知识应用。数据集由选择题组成，设计初衷是“对人类简单、对AI具有挑战性”，用于测试模型在非纯数学领域中应用科学知识与推理的能力。
- **GPQA-D（研究生级抗谷歌问答-钻石级）**：用于评估科学领域高级推理能力的挑战性基准。GPQA包含448道选择题，旨在评估LLM和可扩展监督机制的能力；其中“钻石级（Diamond）”子集含198道难题，聚焦生物、物理、化学领域的“抗谷歌”问题（即即便通过互联网搜索也难以解答），需研究生级专业知识，用于测试模型在复杂场景下的深度领域专长与推理能力。
- **MMLU-Pro（大规模多任务语言理解-专业级）**：原始MMLU基准的增强版本，设计更具挑战性与稳健性，旨在严格评估语言理解能力。原始MMLU基准涵盖数学、历史、法律、医学等57个学科，既评估事实知识，也评估模型在特定场景中应用知识的能力；MMLU-Pro在保持学科广度的同时，提高了难度并减少了潜在解题捷径。

（表A2：本研究使用的基准数据集详情。注：“–”表示官方未提供该划分）

| 数据集                | 训练样本数 | 测试样本数 | 任务类型       | 领域         | 许可证       | 来源       |
|-----------------------|------------|------------|----------------|--------------|--------------|------------|
| 训练数据集            |            |            |                |              |              |            |
| OpenR1-Math-220k      | 220,000    | –          | 数学推理       | 数学         | Apache 2.0   | [链接]     |
| 数学推理基准          |            |            |                |              |              |            |
| AIME24                | –          | 30         | 数学竞赛       | 数学         | MIT          | [链接]     |
| AMC                   | –          | 83         | 数学竞赛       | 数学         | Apache 2.0   | [链接]     |
| MATH500               | –          | 500        | 数学推理       | 数学         | -            | [链接]     |
| Minerva               | –          | 272        | 数学推理       | 数学         | Apache 2.0   | [链接]     |
| Olympiad              | –          | 674        | 数学竞赛       | 数学         | Apache 2.0   | [链接]     |
| 分布外（OOD）基准     |            |            |                |              |              |            |
| ARC-C                 | –          | 1,172      | 科学推理       | 通用科学     | CC-BY-SA-4.0 | [链接]     |
| GPQA-D                | –          | 198        | 科学推理       | 生物、物理、化学 | CC-BY-4.0    | [链接]     |
| MMLU-Pro              | –          | 12,032     | 多任务理解     | 多学科       | MIT          | [链接]     |

# 附录
## D SFT梯度推导
在本节中，我们将为监督微调（SFT）损失函数的梯度提供详细的数学推导，该推导在3.1.1节中已有所提及。

### D.1 问题设定
给定数据集\(D=\{(x_{i}, y_{i})\}_{i=1}^{N}\)（其中\(x_{i}\)为输入提示，\(y_{i}=(y_{i,1}, y_{i,2}, ..., y_{i,T_{i}})\)为对应的目标序列），SFT的目标函数如下（与公式（1）一致）：
$$
\mathcal{L}_{SFT}(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[-\log \pi_{\theta}(y | x)\right] \tag{A3}
$$

由于序列概率可分解为：
$$
\pi_{\theta}(y | x)=\prod_{t=1}^{T} \pi_{\theta}\left(y_{t} | x, y_{<t}\right) \tag{A4}
$$

因此，SFT损失函数可转化为：
$$
\mathcal{L}_{SFT}(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[-\sum_{t=1}^{T} \log \pi_{\theta}\left(y_{t} | x, y_{<t}\right)\right] \tag{A5}
$$

### D.2 梯度推导
为求解梯度\(\nabla_{\theta} \mathcal{L}_{SFT}(\theta)\)，我们需计算：
$$
\nabla_{\theta} \mathcal{L}_{SFT}(\theta)=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[-\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(y_{t} | x, y_{<t}\right)\right] \tag{A6}
$$

此处需把握一个核心洞见：在每一步\(t\)，模型会对整个词汇表\(\mathcal{V}\)中的令牌输出一个概率分布。目标令牌\(y_t\)的对数概率梯度，可通过模型对词汇表中所有令牌的预测概率来表示。

对于位置\(t\)处的任意令牌\(v \in \mathcal{V}\)，有：
$$
\nabla_{\theta} \log \pi_{\theta}\left(v | x, y_{<t}\right)=\frac{1}{\pi_{\theta}\left(v | x, y_{<t}\right)} \nabla_{\theta} \pi_{\theta}\left(v | x, y_{<t}\right) \tag{A7}
$$

由于\(\sum_{v \in \mathcal{V}} \pi_{\theta}(v | x, y_{<t})=1\)，因此存在约束条件：
$$
\sum_{v \in \mathcal{V}} \nabla_{\theta} \pi_{\theta}\left(v | x, y_{<t}\right)=0
$$

结合链式法则以及softmax归一化对所有词汇表令牌的影响，梯度可表示为：
$$
\nabla_{\theta} \log \pi_{\theta}\left(y_{t} | x, y_{<t}\right)=\sum_{v \in \mathcal{V}}\left(1_{v=y_{t}}-\pi_{\theta}\left(v | x, y_{<t}\right)\right) \nabla_{\theta} \log \pi_{\theta}\left(v | x, y_{<t}\right) \tag{A9}
$$
其中，\(1_{v=y_{t}}\)为指示函数——当\(v=y_{t}\)时，该函数值为1；否则为0。

将上述式子代入SFT梯度的表达式中，可得：
$$
\nabla_{\theta} \mathcal{L}_{SFT}=\mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\sum_{t=1}^{|y|} \sum_{v \in \mathcal{V}}\left(\pi_{\theta}\left(v | x, y_{<t}\right)-1_{v=y_{t}}\right) \nabla_{\theta} \log \pi_{\theta}\left(v | x, y_{<t}\right)\right] \tag{A10}
$$

该公式揭示了SFT的核心机制：在每一步中，梯度会促使模型提高目标令牌的概率（当\(1_{v=y_{t}}=1\)时），同时降低词汇表中所有其他令牌的概率。对于每个非目标令牌\(v\)，其概率降低的幅度与其当前概率成正比。

这一分析验证了我们的实证观察结果：SFT会对模型在整个词汇表上的概率分布产生广泛的粗粒度改变，通过系统性地将分布向训练数据中的目标令牌锐化，实现模型对目标序列的拟合。


## E 学习动态可视化细节
为刻画训练过程中的模型动态，在3.1.1节中，我们将语言模型定义为“从输入提示到词汇表输出概率分布”的映射。若两个模型对所有输入提示下的所有输出令牌均分配相同的概率，则认为这两个模型相似或相近。基于这一等价关系，理论上可将每个模型表示为无限维概率空间中的一个点——该空间的每个维度，对应模型在所有可能输出的特定位置上对特定令牌分配的概率。

### E.1 理论定义
#### 模型空间形式化
我们将模型空间\(\mathcal{M}\)定义为所有可能的令牌序列条件概率分布的集合。因此，每个模型\(M \in \mathcal{M}\)可表示为无限维空间中的一个向量，其中每个维度对应词汇表令牌\(v_i\)在特定位置\(t\)上的条件概率。

#### 基于参考模型的模型动态测距
为使该空间可用于分析与可视化，我们通过“模型与一组参考模型\(\mathcal{R}=\{R_1, R_2, ..., R_k\}\)（即参考坐标系）之间的距离”来衡量模型动态：
$$
d_{\mathcal{R}}(M)=\left(d_{R_1}(M), d_{R_2}(M), ..., d_{R_k}(M)\right)^{\top} \tag{A11}
$$
其中，\(d_{R_i}(M)\)通过序列概率衡量模型\(M\)与参考模型\(R_i\)之间的距离。

### E.2 实验设置
为实现上述定义，我们构建了一个数据集：让每个参考模型在相同提示下生成1024条响应序列。这些提示来源于多种数学推理基准（包括AIME24、Minerva、Olympiad、AMC和MATH500）的混合集合，以确保覆盖广泛的问题领域与难度级别。

对于每条响应，我们计算模型\(M\)对参考令牌分配的概率与参考模型分配概率之间的距离，随后对所有响应的距离进行聚合，得到每个参考模型\(R_i\)对应的最终距离$d_{R_i}(M)$。这些距离共同定义了模型在投影子空间中的位置。

我们选取了三个参考模型来构建投影基：（1）DeepSeek-R1（Guo等，2025）——代表当前最先进的推理性能；（2）QwQ-32B（团队，2025）——作为性能优异但结构独特的基准模型；（3）Qwen-2.5-Math-7B（Yang等，2024）——作为微调前的基础模型。这三个模型涵盖了从基础能力到高级能力的范围，构成了具有语义意义的坐标系。

对于训练过程中的每个模型 checkpoint，我们会根据其对参考响应的概率分配来评估距离，从而在模型空间中得到一条轨迹，该轨迹刻画了模型随时间的演化过程。通过对比不同训练范式（如SFT和RL）的轨迹，我们可以发现它们独特的优化动态与收敛行为。

这种三维距离框架既具备理论基础，又具有实际可解释性，可用于分析训练动态。它支持对不同模型变体的直接比较，并能揭示特定训练策略如何影响模型在推理能力空间中的演进路径。