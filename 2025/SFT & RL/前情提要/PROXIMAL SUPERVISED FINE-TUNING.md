# 摘要
基础模型的监督微调（SFT）往往会导致泛化能力不佳，即在新任务或新领域上微调后，模型原有的能力会退化。受强化学习（RL）中信任域策略优化（TRPO）和近邻策略优化（PPO）的启发，我们提出了近邻监督微调（Proximal SFT，简称PSFT）——一种融入了信任域优势的微调目标函数。该函数能在监督微调过程中有效约束策略偏移，同时保持具有竞争力的微调效果。通过将监督微调视为具有恒定正向优势的策略梯度方法的特例，我们推导出的PSFT不仅能使优化过程更稳定、提升模型泛化能力，还能为后续训练后阶段的进一步优化预留空间。在数学和人类价值这两个领域的实验表明：PSFT在域内性能上与SFT相当，在域外泛化能力上则优于SFT；在长时间训练下仍能保持稳定，且不会导致熵坍缩；同时，它还能为后续的优化提供更坚实的基础。

# 1 引言
训练后阶段已成为整个训练过程的关键组成部分。具体而言，强化学习（RL）算法（如PPO（Schulman et al., 2017）和GRPO（Shao et al., 2024））在应用于专注于推理任务的语言模型（LM）时，已展现出显著的有效性。随着RL技术的不断扩展，基础模型能够通过更深入、更广泛的推理来解决复杂问题（OpenAI, 2024; Guo et al., 2025）。这些推理模型在互联网上蕴含着丰富且宝贵的潜在思路（Ruan et al., 2025）。目前，学术界和工业界已开展大量研究，试图通过监督微调（SFT）来利用这些知识（Guha et al., 2025; Li et al., 2025）。与RL相比，SFT作为一种蒸馏方法，具有效率高、实现简单的优势。

但监督微调（SFT）模型常因泛化能力不佳而受到诟病（Huan et al., 2025）。这一局限的根源在于，SFT本质上是一种行为克隆：当微调数据集质量欠佳，或其数据分布与预训练数据分布错位时（Chu et al., 2025），SFT会导致模型泛化能力薄弱，还可能引发大幅策略更新（Schulman et al., 2015）。而强化学习微调（RFT）为解决这一问题提供了可行方案——多项研究表明，与SFT易削弱模型泛化能力不同，RL能更好地保留模型的泛化能力（Chu et al., 2025; Huan et al., 2025）。

另一个需要关注的问题是模型探索能力的保留。在实际应用中，SFT常被用作冷启动步骤，以稳定RL训练过程。RL带来的性能提升，很大程度上源于对预训练和SFT阶段已习得能力的优化（Gandhi et al., 2025）。然而，过度依赖SFT会削弱模型的探索能力（Xie et al., 2024）：这是因为SFT会导致熵坍缩（Cui et al., 2025），进而限制RL训练阶段的探索空间（Yu et al., 2025）。

因此，当前SFT模型面临的核心挑战是如何提升其泛化能力并保留探索能力。为应对这些挑战，本文提出了一种改进的微调策略——近邻监督微调（Proximal Supervised Fine-Tuning, PSFT），该策略可避免模型死记硬背（机械学习），实现更可靠的性能。具体而言，我们建立了SFT与RL之间的理论关联，并基于剪辑代理目标函数提出了一种新的优化目标：该目标函数能借助信任域的优势，约束策略更新过程。

实验结果表明，与标准SFT相比，PSFT在目标任务上的性能与SFT相当，同时能保留模型的通用能力。此外，PSFT还能缓解训练过程中的熵坍缩问题，并在后续RL阶段中，使模型在目标任务和泛化任务上均实现更优性能。我们在数学推理和人类价值对齐两个领域对PSFT进行了评估，充分证明了其在实际应用中的广泛适用性。

本文的贡献如下：
- 我们提出了近邻监督微调（PSFT）——一种针对SFT的优化方法。该方法借鉴PPO的思路，采用剪辑代理目标函数，施加类信任域约束，从而避免过度的策略更新。这种设计能在保证模型在目标任务上性能相当的同时，最大程度保留模型的通用能力。
- PSFT能有效防止SFT过程中的熵坍缩和过拟合问题，从而使后续RL阶段在目标特定任务和通用任务上均能取得更稳健、更优异的结果。
- 我们在不同基础模型、不同任务和不同评估指标下，对PSFT进行了全面验证，证明其有望成为标准SFT的优良替代方案。

# 2 预备知识与研究动机
## 2.1 马尔可夫决策过程公式化
本文在马尔可夫决策过程（Markov Decision Process, MDP）框架下，对语言建模与优化问题进行公式化定义。马尔可夫决策过程由元组\((S, A, P)\)定义，各组件含义如下：
- \(S\)为状态空间，代表部分序列；
- \(A\)为动作空间，代表可能的下一个token；
- \(P(s' | s, a)\)为转移概率，表示在动作\(a\)的作用下，从状态\(s\)转移到状态\(s'\)的概率。

在自回归语言模型（autoregressive LMs）场景中，查询序列\(x:=(x_1, ..., x_m) \in X\)用于初始化状态，响应序列\(y:=(y_1, ..., y_n) \in Y\)通过从策略\(\pi_\theta\)中逐次采样动作（token）生成。给定\(x\)时\(y\)的联合概率可表示为：
$$
\pi_{\theta}(y | x)=\prod_{t=1}^{n} \pi_{\theta}(y_{t} | y_{<t}, x)
$$
其中，\(y_{<t}:=\{y_1, ..., y_{t-1}\}\)，且时刻\(t\)的状态为\(s_t=(x, y_{<t})\)。

## 2.2 作为特殊策略更新的监督微调
### 监督微调（Supervised Fine-tuning）
监督微调的训练目标是最小化模型预测token分布与真实token之间的交叉熵损失。该损失的形式化定义如下：
$$
L^{SFT}(\theta)=-\hat{\mathbb{E}}_{(s_{t}, a_{t}^{*}) \sim \mathcal{D}}\left[\log \pi_{\theta}\left(a_{t}^{*} | s_{t}\right)\right] \quad (1)
$$
其中，\((s_t, a_t^*)\)对从离线数据集\(\mathcal{D}\)中采样得到。

### 策略梯度（Policy Gradient）
与之不同，策略梯度直接从当前策略\(\pi_\theta\)与环境交互产生的轨迹中采样数据。利用策略梯度定理，可将相应的损失函数定义为：
$$
L^{PG}(\theta)=\hat{\mathbb{E}}_{(s_{t}, a_{t}) \sim \pi_{\theta}}\left[\log \pi_{\theta}\left(a_{t} | s_{t}\right) \hat{A}_{t}\right] \quad (2)
$$
其中，\(\hat{A}_t\)为时刻\(t\)的估计优势函数。

从这一视角来看，监督微调可视为策略梯度的一种特殊情况：其数据采样来源于固定的离线数据集\(\mathcal{D}\)，且对于真实动作（ground-truth action），优势函数固定为\(\hat{A}_t=1\)——这一设定本质上是最大化专家token的似然概率。

## 2.3 近邻策略优化
### 信任域策略优化（Trust Region Policy Optimization）
信任域策略优化（TRPO，Schulman et al., 2015）通过利用基于旧策略\(\pi_{\theta_{old}}\)收集的轨迹来最大化代理目标，并引入重要性采样对这些样本进行重加权，以评估新策略\(\pi_\theta\)，其代理目标函数如下：
$$
L^{CPI}(\theta)=\hat{\mathbb{E}}_{(s_{t}, a_{t}) \sim \pi_{\theta_{old}}}\left[\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{old}}\left(a_{t} | s_{t}\right)} \hat{A}_{t}\right]=\hat{\mathbb{E}}_{t}\left[r_{t}(\theta) \hat{A}_{t}\right]
$$
其中，\(r_t(\theta)\)为重要性采样比率，上标\(CPI\)代表保守策略迭代（conservative policy iteration, Kakade & Langford, 2002）。通过对\(\pi_\theta\)与\(\pi_{\theta_{old}}\)之间的KL散度施加信任域约束，TRPO可确保每次策略更新不会与参考策略偏离过大。

### 近邻策略优化（Proximal Policy Optimization）
然而，与TRPO类似，直接对\(L^{CPI}\)施加硬性KL约束在实际优化中难度较大。为解决这一问题，近邻策略优化（PPO，Schulman et al., 2017）对代理目标进行改进：通过惩罚使\(r_t(\theta)\)偏离1过远的策略更新，引入剪辑代理目标，具体形式如下：
$$
L^{CLIP}(\theta)=\hat{\mathbb{E}}_{(s_{t}, a_{t}) \sim \pi_{\theta_{old }}}\left[\min \left(r_{t}(\theta) \hat{A}_{t}, \text{clip}\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right] \quad (4)
$$

这种剪辑机制有效定义了一个“软信任域”：\(r_t(\theta)\)被限制在1附近的小范围内，从而避免破坏性更新。与TRPO相比，PPO能实现相当的稳定性与效率。


# 3 近邻监督微调
近邻监督微调（PSFT）的目标是：在提升监督微调目标任务性能的同时，保留模型的通用能力、防止熵坍缩，并为后续优化预留空间。

## 3.1 从PPO到PSFT
本文重新审视了强化学习中的TRPO与PPO目标函数：两种方法均依赖重要性采样比率\(r_t(\theta)\)对回报进行重加权，同时通过约束该比率防止新策略\(\pi_\theta\)与旧策略\(\pi_{\theta_{old}}\)过度偏离。

将这一思路迁移到监督学习场景（假设所有动作均为“正确动作”，即\(\hat{A}_t>0\)），可将优势函数简化为\(\hat{A}_t=1\)，并由此定义近邻监督微调损失：
$$
L^{PSFT}(\theta )=\mathbb{E}_{(s_{t},a_{t})\sim \mathcal{D}}\left[ \min\left( \frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{old}}\left(a_{t} | s_{t}\right)}, \text{clip}\left( \frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{old}}\left(a_{t} | s_{t}\right)}, 1-\epsilon , 1+\epsilon \right) \right) \right] \quad (5)
$$

该目标函数通过限制新、旧策略概率比，对监督学习更新过程进行正则化。值得注意的是，本文允许旧策略\(\pi_{\theta_{old}}\)（而非固定的初始模型）动态演进——这一设计可直观地抑制token概率的过度自信更新，从而保留模型已有的能力。在在线场景下，该方法与标准监督微调训练等价。

### 热身阶段（Warm-Up）
需注意的是，公式（5）中的\((s_t, a_t)\)从离线数据集\(\mathcal{D}\)中采样。在训练初始阶段，由于\(\pi_{\theta_{old}}\)与\(\mathcal{D}\)的分布尚未对齐，\(r_t(\theta)\)可能导致期望偏差。剪辑机制虽能将策略更新约束在可接受范围内，间接降低这种偏差的负面影响，但仍可引入针对\(\mathcal{D}\)的监督微调热身阶段——通过该阶段使初始策略\(\pi_{\theta_{old}}\)与离线数据集更好地对齐，进一步提升监督微调/近邻监督微调的域内性能。

## 3.2 梯度分析
近邻监督微调的梯度更新被约束在信任域内，与PPO的梯度约束逻辑类似。其梯度可表示为：
$$
\begin{gathered} 
\nabla_{\theta} L^{PSFT}(\theta)=\mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \mathcal{D}}\left[r_{t} \cdot \mathbb{I}_{\text{trust}}\left(r_{t}\right) \cdot \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\right], \\ 
\text{其中}\ \mathbb{I}_{\text{trust}}\left(r_{t}\right)=\begin{cases} 
0 & r_{t}>1+\epsilon, \\ 
1 & \text{其他情况}
\end{cases}
\end{gathered}
$$

梯度分析表明：若训练数据集分布与模型分布偏差显著，这些token将不会产生梯度，从而避免大幅策略更新，维持模型的泛化能力。通常，\(\epsilon\)的最优值设为0.2或0.28；如公式（6）所示，更大的\(\epsilon\)会导致梯度增大（相关进一步分析见5.2节）。

# 4 实验
在4.1节中，我们首先对比了PSFT与标准SFT的训练动态，随后评估了两种方法微调后模型的域内与域外性能，评估重点主要放在数学领域；在4.2节中，我们尝试验证PSFT作为强化学习（RL）冷启动起点的有效性；在4.3节中，我们进一步在对齐领域开展实验，以评估所提方法的普遍适用性。

## 4.1 数学推理领域SFT阶段的主实验
### 实验设置
（1）**模型与数据集**：我们基于Qwen2.5-7B-Instruct（Yang等人，2025）和Llama3.1-8B-Instruct（Dubey等人，2024）两种模型评估所提方法。训练数据聚焦数学领域，旨在通过数学推理提升通用语言模型能力，采用的是OpenR1-Math-8192数据集（Face，2025）——该数据集包含长链思维（chain-of-thought, CoT）样本。  
（2）**基线方法**：我们将标准SFT方法以及一种SFT变体（记为$SFT_{KL}$）作为基线。其中，$SFT_{KL}$在损失函数中融入了KL散度约束，KL正则化系数设为0.5；PSFT中的$\epsilon$参数固定为0.28。更多细节可参见附录A.1.1。

### 4.1.1 训练动态
我们首先分析了各方法的训练动态：对每种方法进行约10轮（epoch）训练，并绘制其熵值与性能的变化曲线。本节中，我们采用AIME-24数据集的avg@32指标衡量域内性能，采用GPQA（Rein等人，2024）数据集的avg@8指标衡量域外性能。

![图1：熵值的训练动态（每178步为1轮）](注：原文此处为图1，包含Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct两种模型的熵值变化曲线，曲线标注分别为SFT、SFT-KL、PSFT、PSFT（热身100步）、PSFT（热身200步）)

**观察1：PSFT可避免熵坍缩**  
图1展示了熵值的演变过程。与SFT和$SFT_{KL}$相比，PSFT的熵值曲线更为平滑；而SFT和$SFT_{KL}$每轮训练后熵值均呈现显著下降趋势，这表明它们存在潜在过拟合风险。这一结果说明，PSFT能够支持长期、token级的细粒度训练，且不会引发熵坍缩。值得注意的是，加入热身阶段的PSFT同样具备这种稳定性，且热身轮次对整体熵值水平具有关键影响（更多证据可参见图6a）。

**分析1：在熵值相近的情况下，PSFT的域内性能与标准SFT相当甚至更优**  
图2展示了域内评估性能的演变过程。PSFT在目标数学任务上实现了与SFT相当的性能，甚至在熵值相近时表现更优。例如，在训练至1300步时，PSFT的AIME24得分接近20，此时熵值约为0.3；而SFT在300-520步区间内的熵值也处于这一水平，但PSFT在该区间内的AIME得分高于SFT。尽管$SFT_{KL}$从直观上可缓解分布偏移问题，但综合熵值趋势与域内性能来看，其效果仍不及PSFT。

（注：原文此处为图2，即“域内性能的训练动态”，包含Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct两种模型的域内性能变化曲线，曲线标注分别为SFT、SFT-KL、PSFT、PSFT（热身））

**分析2：加入热身操作的PSFT性能优于标准SFT**  
图2同样体现了热身阶段对PSFT的影响：加入热身阶段后，PSFT的域内性能稳步提升。在两种模型上，带热身的PSFT均持续优于基础版PSFT及标准SFT基线；且增加热身步数可进一步提升域内性能。这一发现为充分利用s1k（Muennighoff等人，2025）、LIMO（Ye等人，2025）等高质量数据集提供了有效途径，同时可避免熵坍缩问题。

（注：原文此处为图3，即“域外性能的训练动态”，包含Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct两种模型的域外性能变化曲线，曲线标注分别为SFT、SFT-KL、PSFT）

**分析3：PSFT能在很大程度上保留模型的泛化能力**  
图3展示了域外评估性能的演变过程。对于SFT模型，训练步数增加会导致泛化能力下降——例如，Qwen2.5-7B-Instruct模型经SFT训练700步后，性能出现明显下滑。尽管$SFT_{KL}$在一定程度上缓解了这种性能退化，但PSFT的泛化能力表现显著更优。

综上，得益于训练过程中稳健的熵值控制，PSFT避免了策略更新的大幅波动。从域内与域外评估结果均可看出，PSFT的性能总体呈上升趋势；而SFT和$SFT_{KL}$的域外性能则表现出明显波动。

### 4.1.2 详细评估
#### 实验设置
（1）**模型 checkpoint 选择**：为保证对比公平性，我们根据域内性能选择模型 checkpoint。例如，对于Qwen2.5-7B-Instruct模型，我们选用训练700步的SFT模型、训练900步的$SFT_{KL}$模型以及训练1300步的PSFT模型。  
（2）**评估基准**：  
- 域内任务：AIME24、AIME25、AMC、MATH-500（Hendrycks等人，2021）、OlympidBench（He等人，2024）、Minerva（Lewkowycz等人，2022）；  
- 域外任务：GPQA（Rein等人，2024）、ARC-C（Clark等人，2018）、TruthfulQA（Lin等人，2021）、MMLUPro（Wang等人，2024）、SuperGPQA（Du等人，2025）、HeadQA（Vilares & Gómez-Rodríguez，2019）、IFEval（Zhou等人，2023）。  
（3）**推理设置**：推理长度设为10240个token（IFEval任务设为4096个token），top-p参数为0.95，温度（temperature）为0.7。

**分析1：带热身的PSFT在域内任务上持续优于标准SFT**  
表1展示了各方法在不同域内基准上的性能。所有微调方法均较原始模型实现了显著性能提升；而带热身的PSFT不仅如前所述（图2）稳定了早期训练动态，还在不同评估基准上提升了模型稳健性。

**表1：域内性能详细结果（AIME和AMC采用avg@32指标，其余任务采用avg@8指标）**

| 方法                | AIME24 | AIME25 | AMC   | MATH-500 | OlympidBench | Minerva | 平均值 |
|---------------------|--------|--------|-------|----------|--------------|---------|--------|
| Qwen2.5-7B-Instruct |        |        |       |          |              |         |        |
| 原始模型            | 11.25  | 8.75   | 52.58 | 75.05    | 39.72        | 40.53   | 37.98  |
| SFT                 | 22.08  | 23.02  | 62.73 | 84.10    | 52.35        | 43.66   | 47.99  |
| SFT-KL              | 19.27  | 21.56  | 63.20 | 83.55    | 52.70        | 42.19   | 47.08  |
| PSFT（带热身）      | 22.92  | 23.02  | 62.42 | 84.68    | 52.30        | 43.38   | 48.17  |
| PSFT                | 19.38  | 21.98  | 62.34 | 83.35    | 51.50        | 43.33   | 46.98  |
| Llama3.1-8B-Instruct|        |        |       |          |              |         |        |
| 原始模型            | 3.96   | 0.73   | 24.45 | 48.55    | 17.09        | 25.87   | 20.11  |
| SFT                 | 10.63  | 16.77  | 47.19 | 72.60    | 41.43        | 32.17   | 36.80  |
| SFT-KL              | 9.58   | 16.15  | 45.55 | 69.83    | 39.19        | 26.75   | 34.51  |
| PSFT（带热身）      | 12.08  | 18.75  | 49.45 | 74.15    | 42.07        | 33.64   | 38.36  |
| PSFT                | 10.31  | 14.48  | 46.80 | 71.98    | 39.61        | 32.40   | 35.93  |

**分析2：PSFT展现出强劲的泛化能力**  
表2展示了各方法在不同域外基准上的性能。总体而言，实验结果与Zhou等人（2025）的研究结论一致：向模型注入长CoT数据可提升其通用推理能力。然而，在用于评估指令遵循能力的IFEval任务上，SFT、$SFT_{KL}$及带热身的PSFT均导致原始模型性能显著退化；与之相反，PSFT不仅保留了模型的指令遵循能力，还在其他领域大幅提升了模型的推理能力。此外，带热身的PSFT在域外任务上同样持续优于标准SFT。

**表2：域外性能详细结果（GPQA、ARC-C、TruthfulQA、IFEval采用avg@8指标，其余任务采用pass@1指标）**

| 方法                | GPQA   | ARC-C  | TruthfulQA | MMLU-Pro | SuperGPQA | HeadQA | IFEval loose | 平均值 |
|---------------------|--------|--------|------------|----------|-----------|--------|--------------|--------|
| Qwen2.5-7B-Instruct |        |        |            |          |           |        |              |        |
| 原始模型            | 31.38  | 91.54  | 66.10      | 54.99    | 27.59     | 73.41  | 73.94        | 59.85  |
| SFT                 | 32.89  | 92.22  | 63.14      | 58.98    | 29.02     | 74.65  | 54.42        | 57.90  |
| SFT-KL              | 32.95  | 91.86  | 61.31      | 58.35    | 27.69     | 74.07  | 55.44        | 57.38  |
| PSFT（带热身）      | 33.27  | 92.09  | 66.37      | 59.28    | 28.37     | 75.24  | 55.07        | 58.53  |
| PSFT                | 33.21  | 92.29  | 67.16      | 59.18    | 28.10     | 75.82  | 73.03        | 61.26  |
| Llama3.1-8B-Instruct|        |        |            |          |           |        |              |        |
| 原始模型            | 24.62  | 80.96  | 55.14      | 43.70    | 18.16     | 68.20  | 78.10        | 52.70  |
| SFT                 | 19.38  | 87.73  | 67.08      | 50.29    | 21.95     | 73.52  | 33.45        | 50.49  |
| SFT-KL              | 18.18  | 87.02  | 67.03      | 47.02    | 19.81     | 71.95  | 34.19        | 49.31  |
| PSFT（带热身）      | 23.99  | 89.72  | 68.55      | 56.03    | 25.26     | 77.71  | 33.08        | 53.48  |
| PSFT                | 26.89  | 89.11  | 68.69      | 56.58    | 25.85     | 77.90  | 69.75        | 59.25  |

**分析3：PSFT在不同模型上均展现出稳健性**  
例如，在评估GPQA和TruthfulQA任务时，SFT和$SFT_{KL}$在不同模型上的表现不一致（有时某一方法会损害性能，而另一方法可能提升性能）；与之相反，PSFT在两种模型上均持续提升性能，尤其在Llama模型上展现出显著优势。


## 4.2 强化学习阶段模型潜力的探索
实际应用中，语言模型（LLM）通常在SFT之后进行强化学习（RL）训练。经监督微调的模型需作为后续RL阶段的良好起点，既要避免过拟合与欠拟合，又要能更好地激发RL的优化潜力。本节中，我们评估了PSFT在RL阶段的作用。

### 实验设置
我们采用了DAPO（Yu等人，2025）中的所有技术，将clip-higher参数设为0.28。RL训练使用DAPO-MATH-17k数据集（Shao等人，2024），详细训练配置可参见附录A.1.2。评估时，我们同样根据域内最高性能选择模型checkpoint。

### 4.2.1 强化学习阶段的训练动态
（注：原文此处为图4，即“RL实验中的熵值训练动态”，包含Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct两种模型的熵值变化曲线，曲线标注分别为PSFT+GRPO、SFT+GRPO）

**分析1：PSFT为RL优化预留了更大空间**  
图4展示了RL阶段的熵值演变过程。低熵值会限制模型的探索能力，值得注意的是，以PSFT作为冷启动起点时，模型在整个RL训练过程中均保持更高的熵值，且熵值上升趋势更陡峭。

（注：原文此处为图5，即“RL实验中的域内性能训练动态”，包含Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct两种模型的域内性能变化曲线，曲线标注分别为PSFT+GRPO、SFT+GRPO）

**分析2：PSFT在RL阶段呈现“起步慢、追赶快”的特点**  
图5展示了RL阶段域内性能的演变过程。得益于高熵值带来的更强探索能力，以PSFT作为冷启动起点的模型，其性能提升曲线更陡峭，最终超过了以SFT作为冷启动起点的模型性能。

### 4.2.2 详细评估
**分析3：RL后PSFT的域内性能更优**  
表3展示了RL实验在域内基准上的结果。总体而言，RL提升了所有模型的性能；对域内任务的综合评估表明，尽管PSFT在初始阶段（SFT后）性能落后于SFT，但其具备巨大的优化潜力，可通过RL进一步释放。

**表3：RL实验的域内性能详细结果（AIME和AMC采用avg@32指标，其余任务采用avg@8指标）**

| 方法                | AIME24 | AIME25 | AMC   | MATH-500 | OlympidBench | Minerva | 平均值 |
|---------------------|--------|--------|-------|----------|--------------|---------|--------|
| Qwen2.5-7B-Instruct |        |        |       |          |              |         |        |
| SFT                 | 22.08  | 23.02  | 62.73 | -        | 52.35        | 43.66   | 47.99  |
| → GRPO              | -      | 26.15  | 70.86 | 86.60    | 58.09        | 45.27   | 52.40  |
| PSFT                | -      | 21.98  | -     | 83.35    | 51.50        | 43.33   | 46.98  |
| → GRPO              | 28.13  | 27.19  | 71.72 | 87.48    | 58.50        | -       | -      |
| Llama3.1-8B-Instruct|        |        |       |          |              |         |        |
| SFT                 | 10.63  | -      | 47.19 | 72.60    | 41.43        | -       | 36.80  |
| → GRPO              | -      | 17.50  | -     | 77.03    | -            | 33.09   | 40.53  |
| PSFT                | 10.31  | 14.48  | 46.80 | 71.98    | 39.61        | 32.40   | -      |
| → GRPO              | 14.27  | -      | 55.23 | 77.78    | 47.74        | 37.55   | -      |

**分析4：RL后PSFT在域外任务上的性能远超SFT**  
表4展示了RL实验在域外基准上的结果。显然，若模型在冷启动阶段（SFT后）能力较弱，后续RL训练会受此限制——例如，IFEval任务的结果清晰地体现了这一点。这一现象进一步凸显了改进现有SFT算法的必要性。

**表4：RL实验的域外性能详细结果（GPQA、ARC-C、TruthfulQA、IFEval采用avg@8指标，其余任务采用pass@1指标）**

| 方法                | GPQA   | ARC-C  | TruthfulQA | MMLU-Pro | SuperGPQA | HeadQA | IFEval loose | 平均值 |
|---------------------|--------|--------|------------|----------|-----------|--------|--------------|--------|
| Qwen2.5-7B-Instruct |        |        |            |          |           |        |              |        |
| SFT                 | 32.89  | 92.22  | 63.14      | -        | -         | 74.65  | 54.42        | 57.90  |
| → GRPO              | 39.39  | 92.25  | -          | -        | 33.80     | 75.42  | -            | -      |
| PSFT                | -      | 92.29  | 67.16      | 59.18    | 28.10     | 75.82  | 73.03        | 61.26  |
| → GRPO              | 43.43  | -      | 64.84      | 63.65    | -         | -      | 73.73        | -      |
| Llama3.1-8B-Instruct|        |        |            |          |           |        |              |        |
| SFT                 | -      | 87.73  | -          | 50.29    | 21.95     | 73.52  | -            | -      |
| → GRPO              | 31.06  | 88.57  | -          | -        | -         | 73.81  | 31.85        | 53.19  |
| PSFT                | 26.89  | 89.11  | 68.69      | 56.58    | -         | -      | -            | 59.25  |
| → GRPO              | -      | -      | 65.24      | 60.92    | 31.30     | 78.12  | 71.02        | 61.96  |


## 4.3 人类对齐领域的进一步实验
为验证PSFT的通用性，本节将实验扩展到人类对齐数据集，并采用不同基础模型与算法（如DPO（Rafailov等人，2023））。实验结果表明，PSFT能有效降低“对齐损失”（即泛化差距），同时仍为后续优化预留空间。

### 实验设置
为进一步验证PSFT的通用性，我们采用了完全不同的实验设置：  
（1）**模型与数据集**：基于预训练的Qwen3-4B-Base模型（Yang等人，2025），使用UltraFeedback数据集（Cui等人，2023）进行微调。  
（2）**训练算法**：首先在数据集中选取特定子集，分别用SFT/PSFT对模型进行微调（其中PSFT额外设置了“延长训练”版本，记为$PSFT_{prolong}$）；随后采用DPO算法，使模型学习对比奖励信号。  
（3）**评估**：由于短CoT模型无法解决复杂问题，我们采用ARC、GSM8K（Cobbe等人，2021）、MMLU（Hendrycks等人，2020）、GPQA、TruthfulQA等任务评估对齐损失；在MT-Bench（Zheng等人，2024a）、AlpacaEval（Dubois等人，2024）、Arena-Hard（Li等人，2024a）三个对齐基准上评估模型对齐性能。评估时，采用Qwen3-30B-A3-Instruct-2507模型（Yang等人，2025）作为评判模型，并使用llm-eval-harness（Gao等人，2024）评估对齐损失性能。更多实验设置可参见附录A.1.3。

（注：原文此处为图6，即“SFT/PSFT后续DPO训练的动态”，包含图6a“熵值演变”和图6b“DPO训练中正负奖励变化”，曲线标注分别为SFT、PSFT、PSFT_{prolong}）

### 4.3.1 训练动态
**分析1：PSFT在其他领域仍能可靠避免熵坍缩**  
如图6a所示，SFT的熵值损失仍呈现明显的“锯齿状”波动，表明其存在潜在过拟合现象；而PSFT则克服了这一问题。因此，SFT会导致严重的对齐损失（见图7），并限制后续优化（见表5）。

### 4.3.2 人类对齐性能
**分析2：PSFT在对齐基准上表现更优**  
表5展示了DPO实验在对齐基准上的结果。可以观察到，$PSFT_{prolong}$在实现与SFT相近训练效果的同时，避免了熵坍缩，且在DPO训练阶段表现更优；此外，以PSFT作为冷启动起点能带来额外收益，且该收益可通过DPO进一步释放。这一结果与Xiao（2024）的研究结论一致：当SFT模型发生坍缩（即模型输出过度偏向某些结果，确定性接近100%，见图6b）时，会在后续对齐过程中引发“偏好坍缩”。

**表5：Qwen3-4B-Base模型在对齐基准上的DPO训练结果**

| 方法                | LC（%） | WR（%） | AlpacaEval2 | Arena-Hard WR（%） | 1-turn MT-Bench | 2-turn MT-Bench |
|---------------------|---------|---------|-------------|-------------------|-----------------|-----------------|
| SFT                 | 12.24   | 8.52    | 17.90       | 7.64               | 7.64            | 5.71            |
| → DPO               | 16.96   | 13.40   | 26.50       | 7.91               | 7.91            | 6.00            |
| $PSFT_{prolong}$    | 11.95   | 8.37    | 17.40       | 7.41               | 7.41            | 5.84            |
| → DPO               | 19.26   | 15.17   | 30.20       | 7.63               | 7.63            | 6.74            |
| PSFT                | 11.79   | 7.49    | 11.40       | 6.83               | 6.83            | 4.86            |
| → DPO               | 23.29   | 20.13   | 36.40       | 8.51               | 8.51            | 6.95            |

**分析3：PSFT训练能充分利用正负样本**  
如图6b所示，以SFT或PSFT作为冷启动起点，会对后续DPO阶段产生影响：$PSFT_{prolong}$与PSFT在正样本上的奖励相近，主要差异在于延长训练减少了负样本的出现；与之相反，SFT训练会导致正样本频繁出现而负样本罕见，这限制了模型从奖励信号中学习的能力，反而使模型偏向人类选择的数据。

**分析4：PSFT能有效降低对齐损失**  
图7展示了以GSM8K、MMLU、GPQA等任务作为域外任务时的对齐损失结果，表明：（a）与原始SFT相比，PSFT及PSFT+DPO在SFT阶段能更大程度保留模型的通用能力；（b）即使$PSFT_{prolong}$因可能的过拟合问题导致性能略有下降，其在对齐损失上仍取得了相对较好的结果，这体现了PSFT在实际场景中的稳健性。

（注：原文此处为图7，即“模型在对齐损失（域外任务）上的结果”，横坐标为模型（Base、PSFT、PSFT+DPO、PSFT-Prolong、PSFT-Prolong+DPO、SFT、SFT+DPO），纵坐标为各任务性能，任务包括CMMLU、GPQA、MMLU、GSM8K、TruthfulQA）

# 5 深度分析
## 5.1 PSFT中的剪辑token
图8展示了PSFT训练过程中具有代表性的剪辑token。可以看出，剪辑token主要集中在“wait（等待）”“alternatively（或者）”等表达不确定性的词汇上，这类词汇体现了某种“长期思维模式”，而这些思维模式本应在后续RL阶段中学习。随着训练的推进，这些token的剪辑权重愈发显著，而其他token的剪辑权重则逐渐减小。通过PSFT，这种“思维模式”被平稳地融入模型中，且未对模型的通用能力造成明显干扰。

（注：原文此处为图8，即“PSFT训练过程中剪辑token的示例与变化”，包含图8a“第1轮中的剪辑token云图”和图8b“第3轮中的剪辑token云图”，展示了不同训练轮次下被剪辑token的分布差异）


## 5.2 剪辑值的参数分析
如图9所示，无剪辑的PSFT虽能保持较高熵值，但存在梯度范数过大且不稳定、下游结果波动剧烈的问题。通过引入信任域剪辑机制，PSFT实现了更优的平衡：既防止了熵坍缩，稳定了梯度更新，又实现了性能的稳步提升。在不同剪辑阈值下，中等大小的$\epsilon$值（如0.28）在稳定性与性能之间展现出尤为理想的权衡效果。

（注：原文此处为图9，即“不同剪辑值下PSFT的域内结果”，包含图9a“熵值”、图9b“梯度范数”、图9c“性能”三条曲线，展示了不同剪辑参数对模型关键指标的影响）


# 6 相关工作
## 监督微调（Supervised Fine-tuning）
监督微调通常是训练后阶段（post-training pipeline）的初始步骤，解决SFT阶段的潜在问题可显著提升后续微调模型的稳健性。已有多项研究指出，标准交叉熵（CE）损失可能并非SFT的最优目标函数（Li et al., 2024b；Xiao, 2024），认为该阶段使用CE损失易导致模型记忆训练数据，而非习得更具泛化性的能力。

Li等人（2024b）提出了GEM方法——一种基于博弈论的SFT算法，通过熵正则化来保留多样性并缓解遗忘问题。同期相关工作包括重要性加权SFT（iw-SFT）（Qin & Springenberg, 2025）和DFT（Wu et al., 2025）：iw-SFT的核心思路是将标准SFT重新解读为在稀疏奖励场景下对RL目标函数的松散下界优化，而 vanilla SFT仅最大化经过筛选的“成功”轨迹的似然概率，随着模型分布与参考策略偏离，该下界会愈发松散；为此，iw-SFT通过引入重要性重加权，为更优轨迹分配更高权重，从而使下界更接近真实RL目标。DFT则将SFT视为一种有缺陷的策略梯度，通过引入简单的基于概率的重加权来提升泛化能力、。

## 强化学习（Reinforcement Learning）
在SFT之后，RL常被用于直接优化奖励信号。策略梯度方法（Sutton et al., 2000）是RL的基础，但存在稳定性差、方差高的问题。为解决这一缺陷，TRPO（Schulman et al., 2015）引入了信任域约束，将每次策略更新限制在KL散度较小的邻域内，以确保稳定提升；PPO（Schulman et al., 2017）则通过剪辑代理目标进一步简化该方法，在稳定性与效率之间取得了平衡。


# 7 结论
受TRPO与PPO的启发，本文提出了近邻监督微调（Proximal Supervised Fine-Tuning, PSFT）方法。PSFT在保持与标准SFT相当的域内性能（熵值相近时）的同时，能维持平滑的熵值曲线，并展现出强劲的泛化能力。此外，将PSFT作为冷启动模型，可为强化学习（RL）优化、直接偏好优化（DPO）等后续训练后技术提供更有效的基础。综上，PSFT为传统SFT提供了一种极具潜力的替代方案。

# 附录A
## A.1 实验细节
本研究采用verl框架（Sheng等人，2024）进行监督微调（SFT）、近邻监督微调（PSFT）及强化学习（RL）训练，并使用LLama-Factory（Zheng等人，2024b）开展直接偏好优化（DPO）训练。在verl框架中，损失通过token均值（token-mean）进行聚合。对于SFT与PSFT，权重衰减（weight decay）设置为0.1，所有实验均采用全参数微调（full fine-tuning）方式。

### A.1.1 数学推理实验
SFT与PSFT的详细训练配置如表6所示。

**表6：PSFT/SFT实验配置**
| 方法   | 小批量大小（Mini batch size） | 学习率（Learning rate） | 高剪辑阈值（High clip） | 截断长度（Cutoff_len） |
|--------|-------------------------------|-------------------------|-------------------------|------------------------|
| PSFT   | 32                            | 1e-6                    | 0.28                    | -                      |
| SFT    | 256                           | 2e-5                    | -                       | 10k                    |


### A.1.2 RL阶段模型潜力探索
RL训练的详细配置如表7所示，Qwen系列模型与Llama系列模型采用相同配置。

**表7：RL实验配置**
| 配置项（Config）          | SFT + GRPO | PSFT + GRPO |
|---------------------------|------------|-------------|
| 学习率（lr）              | 1e-6       | 1e-6        |
| KL系数（kl coef）         | 0.0        | 0.0         |
| 最大提示长度（max prompt length） | 2k         | 2k          |
| 最大响应长度（max response length） | 10k        | 10k         |
| 超长缓冲区长度（overlong buffer.len） | 2k         | 2k          |
| 训练批量大小（train batch size） | 256        | 256         |
| PPO小批量大小（ppo mini batch size） | 32         | 32          |
| 低剪辑比率（clip ratio low） | 0.2        | 0.2         |
| 高剪辑比率（clip ratio high） | 0.28       | 0.28        |
| 温度参数（temperature）   | 1.0        | 1.0         |
| 轨迹采样次数（rollout.n） | 8          | 8           |
| 总训练步数（total training steps） | 100        | 100         |


### A.1.3 对齐实验
#### 训练配置
对齐任务中SFT与PSFT的详细训练配置如表8所示，DPO的详细训练配置如表9所示。

**表8：PSFT/SFT实验配置**
| 方法          | 训练批量大小（Train batch size） | 小批量大小（Mini batch size） | 学习率（Learning rate） | 训练轮次（Train epochs） | 高剪辑阈值（High clip） | 截断长度（Cutoff len） |
|---------------|----------------------------------|-------------------------------|-------------------------|--------------------------|-------------------------|------------------------|
| PSFT          | 256                              | 32                            | 1e-6                    | 5                        | 0.28                    | 6k                     |
| PSFT延长训练（PSFT prolong） | 256                              | 32                            | 1e-6                    | 10                       | 0.28                    | 6k                     |
| SFT           | 256                              | -                             | 2e-5                    | 5                        | -                       | 6k                     |

**表9：DPO实验配置**
| 方法   | 训练批量大小（Train batch size） | β    | 学习率（Learning rate） | 训练轮次（Train epochs） | 截断长度（Cutoff len） |
|--------|----------------------------------|------|-------------------------|--------------------------|------------------------|
| DPO    | 64                               | 0.01 | 5e-7                    | 1                        | 4k                     |

#### 评估配置
评估生成采用表10所列的参数设置。为评估对齐损失（alignment tax），本研究使用llm-eval-harness工具集中的对应任务，具体任务设置如表11所示。

**表10：对齐基准生成配置**
| 配置项（Config） | MT-Bench | Alpaca-Eval | Arena-Hard |
|------------------|----------|-------------|------------|
| 温度参数（temperature） | 0.0      | 0.7         | 1.0        |
| 累积概率（top p） | -        | 0.95        | 0.7        |

**表11：基于llm-eval-harness的对齐损失评估**
| 任务（task） | GSM8K          | MMLU                          | TruthfulQA       | CMMLU  | GPQA            |
|--------------|----------------|-------------------------------|------------------|--------|-----------------|
| 设置（setting） | GSM8K链式思维（cot） | MMLU FLAN n-shot生成式（generative） | TruthfulQA多项选择1（mc1） | CMMLU  | 链式思维n-shot（cot n shot） |


## A.2 系统提示
### 推理任务
推理任务采用如下系统提示：请逐步推理，并将最终答案放在\boxed{}内。

### 对齐任务
对齐任务采用如下系统提示：你是一个有帮助的助手。