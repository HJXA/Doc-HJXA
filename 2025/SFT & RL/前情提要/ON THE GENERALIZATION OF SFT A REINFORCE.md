# 摘要
我们针对大语言模型（Large Language Model, LLM）的监督微调（Supervised Fine-Tuning, SFT）提出了一种简单但具有理论依据的改进方法，以解决其相较于强化学习（Reinforcement Learning, RL）泛化能力有限的问题。通过数学分析，我们发现标准SFT的梯度隐含编码了一种有问题的奖励结构，这种结构可能会严重限制模型的泛化能力。为修正这一问题，我们提出了动态微调（Dynamic Fine-Tuning, DFT）方法：通过利用token的概率对目标函数进行动态缩放，来稳定每个token的梯度更新。值得注意的是，这一仅需一行代码的改动，在多个具有挑战性的基准测试和基础模型上均显著优于标准SFT，展现出大幅提升的泛化能力。此外，我们的方法在离线强化学习场景中也取得了具有竞争力的结果，为相关任务提供了一种有效且更简洁的替代方案。本研究搭建了理论洞见与实践解决方案之间的桥梁，显著推动了SFT的性能提升。相关代码将在https://github.com/yongliang-wu/DFT 发布。

# 1 引言
监督微调（Supervised Fine-Tuning, SFT）通过在专家演示数据集上训练模型，已成为大语言模型（Large Language Model, LLM）后训练的标准方法，可用于使模型适应新任务或增强现有能力（Chung et al., 2024; Zhang et al., 2024; Sanh et al., 2022; Ouyang et al., 2022）。它之所以被广泛采用，主要是因为易于实现且能快速习得类专家行为（Wei et al., 2022; Zhou et al., 2023）。然而，尽管具备这些优势，与强化学习（reinforcement learning, RL）方法相比，SFT的泛化能力通常较弱（Chu et al.; Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022; Huan et al., 2025; Swamy et al., 2025）。RL会利用显式的奖励或验证信号，使模型能够探索多样的策略，从而实现更强的泛化能力。尽管如此，RL方法往往需要大量计算资源，对超参数调优敏感，且依赖奖励信号的可获得性——而这些条件在实际场景中并非总能满足（Schulman et al., 2017; Ouyang et al., 2022; Sheng et al., 2025; Strubell et al., 2019; Liu & Yin, 2024; Winsta, 2025）。即便RL可行，在快速习得专家行为模式方面，SFT仍具有优势——这类模式是RL可能难以独立发现的（Mandlekar et al., 2022; Chen et al., 2025b）。

为了利用两种方法的互补优势，研究人员已开发出多种融合SFT与RL的混合方法（Ouyang et al., 2022; Sheng et al., 2025; Rafailov et al., 2023; Liu et al., 2025; Qiu et al., 2025）。但问题在于：我们能否从根本上改进SFT本身？这一问题至关重要，因为当数据集中不存在负样本、且无可用的奖励或验证模型时，SFT是唯一可行的方法。

在本研究中，我们通过数学分析阐明SFT与RL之间的根本差异，从而填补这一研究空白。我们证明，SFT的梯度更新可解释为一种策略梯度方法的特例，且该特例具有特定的、隐含定义的奖励结构。我们的分析表明，这种隐含奖励不仅极其稀疏，还与策略对专家行为的赋值概率成反比（具体数学细节参见公式6）。因此，这会导致一种不适定的（ill-posed）奖励结构——尤其是当模型对专家行为赋值低概率时，由此产生的梯度会出现无界方差，形成病态的优化景观（pathological optimization landscape）。

基于这一数学分析，我们提出了动态微调（Dynamic Fine-Tuning, DFT）方法——这是一种具有理论依据的解决方案，能够从根源上解决隐含奖励结构不适定的问题。对于每个token，我们的方法只需利用该token的概率对标准SFT目标函数进行缩放，即可有效中和导致异常奖励结构与无界方差的逆概率加权。这种基于理论的修改，从根本上将梯度估计器从一种不稳定、有偏且依赖概率的机制，转变为一种稳定、均匀加权的更新过程。

我们方法的实证结果极具说服力。例如，在NuminaMath数据集（LI et al., 2024）上对Qwen2.5-Math系列模型（Qwen Team et al., 2024b）进行微调时，我们的方法相对于基准模型的性能提升，通常是SFT方法提升幅度的数倍。关键在于，当标准SFT在挑战性数据集（包括2024年国际数学奥林匹克基准（International Mathematical Olympiad, 2024）、2024年美国数学邀请赛（AIME 2024, American Institute of Mathematics, 2024）和2023年美国数学竞赛（AMC 2023, Mathematical Association of America, 2023））上因过拟合出现性能下降时，我们的方法始终能实现显著提升，彰显出更强的泛化能力。这一结果在不同模型类型、模型规模和数据规模下均得到了验证（参见表1、图1）。

此外，我们还探索了该方法在RL场景中的适用性（参见表3）——此类场景中存在负样本或密集奖励信号（Levine et al., 2020）。实验表明，我们的方法不仅显著优于直接偏好优化（DPO, Rafailov et al., 2023）、拒绝采样微调（RFT/RAFT, Dong et al., 2023; Ahn et al., 2024）等离线RL方法，在针对Qwen2.5-Math-1.5B模型的数学任务中，其性能还优于广义近端策略优化（GRPO）、近端策略优化（PPO）等在线RL方法。需注意的是，我们的方法在计算资源或流程复杂度上远低于大多数RL方法（后者通常需要参考模型或特定批次大小），因此提供了一种更实用的替代方案。

为了理解DFT对模型的独特影响，我们分析了训练后模型概率分布的变化（参见图2）。我们发现，传统SFT训练会通过均匀提高token概率，使模型更紧密地拟合训练数据；而有趣的是，我们的方法还会将部分token分布推离训练集。具体而言，尽管大多数token仍会更贴近训练集，但拟合程度较弱的token比例显著增加。我们将在4.3.1节中对这一现象进行深入讨论。

本研究的贡献涵盖理论与实践两个层面。在理论层面，我们通过数学推导将LLM的SFT确立为策略梯度空间中的一种特殊RL形式，明确了SFT泛化能力有限的根本原因，并推导得出一种改进方法。在实验层面，我们证明，仅需一行代码的简单修改，就能在各类任务和模型上显著提升LLM的SFT性能与泛化能力。

# 2 相关工作
监督微调（SFT）与强化学习（RL）之间的权衡是现代语言模型对齐领域的核心议题。SFT因能简便高效地模仿专家演示而被广泛采用（Chung et al., 2024; Zhou et al., 2023; Wei et al., 2022），这一过程类似于机器人领域中的行为克隆（Sammut, 2011; Mandlekar et al., 2022）。然而，文献中频繁指出，与利用奖励信号探索并发现更稳健策略的RL相比，SFT容易导致过拟合且泛化能力较差（Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022; Swamy et al., 2025）。近期，Chu等人在文本和视觉任务上对SFT与RL进行了系统性对比，证实了“**SFT侧重记忆，而RL侧重泛化**”这一结论。更重要的是，他们还发现，SFT作为初始化步骤对稳定输出格式至关重要，唯有如此，后续的RL训练才能有效进行。尽管如此，RL仍面临显著的实际障碍，包括高昂的计算成本、超参数敏感性，以及对显式奖励函数的依赖——这些因素往往限制了其应用范围（Schulman et al., 2017; Strubell et al., 2019; Sheng et al., 2025）。

为充分利用两种范式的优势，主流研究方向聚焦于开发混合方法。其中最成熟的策略是：先进行SFT预训练，再通过基于学习到的奖励模型进行RL优化——这一思路因InstructGPT（Ouyang et al., 2022）而得到普及。近期研究还探索了其他组合方式，例如交替进行SFT与RL步骤以提升稳定性和性能（Sheng et al., 2025; Liu et al., 2025; Qiu et al., 2025）。另一些主流方法（如直接偏好优化（DPO）（Rafailov et al., 2023））则绕过显式奖励建模，直接在偏好数据上优化策略，将模仿信号与强化信号整合到单一损失函数中。Chen等人（2025a）提出的负样本感知微调（NFT），通过隐含的负策略对模型自身的错误生成进行建模，使大语言模型能够实现自我改进。尽管这些方法效果显著，但它们均针对“存在奖励信号、偏好对或负样本”的场景设计，虽能增强训练流程，却无法在SFT的原生场景（即仅存在正向专家演示）中从根本上改进SFT过程。本研究的不同之处在于，我们聚焦于**直接改进SFT本身**，无需任何外部反馈。

另一方面，已有理论研究试图建立SFT与RL的统一框架。例如，Du等人（2025）将基于人类反馈的强化学习（RLHF）重构为一种带奖励加权的SFT形式，在简化流程的同时，仍依赖显式奖励。Wang等人（2025）证明SFT可被视为一种具有隐含奖励的RL方法，并提出采用更小学习率等方案来处理原本会消失的KL约束。Abdolmaleki等人（2025）分析了从正负反馈中学习的过程，揭示了正负反馈的平衡对策略收敛的影响。Qin与Springenberg（2025）将SFT视为RL的下界，并通过引入基于数据生成策略的重要性加权来改进SFT。尽管这些研究从加权角度指出了SFT与RL之间的普遍关联，但**未能提供SFT梯度与离线策略梯度之间精确的数学等价关系**。与之相反，本研究首次严格确立了这种等价关系，并明确指出关键差异在于SFT中存在的逆概率加权项。这一洞见直接启发了我们提出的解决方案：只需将SFT损失与模型概率相乘，即可抵消该加权项的影响。

有趣的是，我们提出的方法所设计的交叉熵（CE）损失，与著名的焦点损失（Focal Loss）（Lin et al., 2017）呈截然相反的思路。我们修改后的交叉熵损失为$\mathcal{-p \log (p)}$，而焦点损失为$\mathcal{-(1-p)^{\gamma} \log (p)}$。焦点损失通过刻意降低分类效果好的样本权重，以提升对少数类样本的性能；而我们则通过刻意降低分类效果差的样本权重，以提升模型的泛化能力。这种差异或许反映了大语言模型时代的一个根本性转变：在这一时代，欠拟合已不再是主要问题，过拟合反而更为突出。

# 3 方法
## 3.1 预备知识
### 监督微调（SFT）
设$\mathcal{D}=\{(x, y^{*})\}$表示专家演示语料库，其中$y^{*}$是查询$x$对应的完整参考响应。监督微调（SFT）的目标是最小化句子级交叉熵，其损失函数定义为：  
 $$\mathcal{L}_{SFT}(\theta)=\mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[-log \pi_{\theta}\left(y^{*} | x\right)\right] . $$

其梯度为：  
 $$\nabla_{\theta} \mathcal{L}_{SFT}(\theta)=\mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[-\nabla_{\theta} log \pi_{\theta}\left(y^{*} | x\right)\right] . $$

### 强化学习（RL）
设$y$为从策略$\pi_{\theta}(\cdot | x)$中采样得到的、对应查询$x$的响应。给定奖励函数$r(x, y) \in \mathbb{R}$，策略目标函数定义为：  
 $$J(\theta)=\mathbb{E}_{x \sim \mathcal{D}_{x}, y \sim \pi_{\theta}(\cdot | x)}[r(x, y)] . $$

其句子级策略梯度为：  
 $$\nabla_{\theta} J(\theta)=\mathbb{E}_{x \sim \mathcal{D}_{x}, y \sim \pi_{\theta}(\cdot | x)}\left[\nabla_{\theta} log \pi_{\theta}(y | x) r(x, y)\right] . $$


## 3.2 统一SFT与RL的梯度表达式
### 通过重要性采样将SFT梯度重写为策略梯度
公式2（即SFT梯度公式）是在固定演示分布下计算的。我们通过引入一个重要性权重（用于比较专家分布（狄拉克delta分布）与模型分布），将其转换为在线策略期望，具体形式如下：  
 $$\mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[-\nabla_{\theta} log \pi_{\theta}\left(y^{*} | x\right)\right]=\mathbb{E}_{x \sim \mathcal{D}_{x}} \underbrace{\mathbb{E}_{y \sim \pi_{\theta}(\cdot | x)} \frac{\mathbb{I}\left[y=y^{*}\right]}{\pi_{\theta}(y | x)}\left[-\nabla_{\theta} log \pi_{\theta}(y | x)\right]}_{重采样+重加权 } (5) $$

其中，$\mathbb{I}\left[y=y^{*}\right]$为指示函数（当$y=y^{*}$时取值为1，否则为0）。

定义辅助变量：  
 $$w(y | x)=\frac{1}{\pi_{\theta}(y | x)}, r(x, y)=\mathbb{I}\left[y=y^{*}\right], $$

将公式5用上述辅助变量重新整理后，可得到如下形式：  
 $$\nabla_{\theta} \mathcal{L}_{SFT}(\theta)=-\mathbb{E}_{x \sim \mathcal{D}_{x}, y \sim \pi_{\theta}(\cdot | x)}\left[w(y | x) \nabla_{\theta} log \pi_{\theta}(y | x) r(x, y)\right] . (6) $$

此时，SFT梯度的形式与公式4（即RL策略梯度公式）高度一致。由此可见，传统SFT本质上是一种在线策略梯度方法——其奖励为“匹配专家轨迹的指示函数”，但受限于一个重要性权重$1/\pi_{\theta}$的偏倚影响。

在SFT场景中，奖励信号的稀疏性不可避免，而我们发现，重要性采样权重$1/\pi_{\theta}$正是导致SFT泛化能力弱于RL的根本原因之一。当模型对专家响应的赋值概率较低时，权重$w$会显著增大，从RL的视角来看，这将导致奖励估计出现无界性和高方差问题。而奖励函数的极端稀疏性（仅当模型输出与专家输出完全匹配时，$r(x, y)=\mathbb{I}\left[y=y^{*}\right]$才非零）会进一步加剧这一问题。最终，优化过程会倾向于过拟合到罕见的“完全匹配演示样本”，从而损害模型对训练数据之外场景的泛化能力。


## 3.3 提出的方法：基于动态重加权的奖励修正
为解决从RL目标视角观察到的SFT奖励偏倚问题，我们通过“动态重加权”对奖励进行修正——具体而言，用策略概率$1/w$（即$w$的逆比率）与奖励相乘。由此得到的“动态微调（DFT）”梯度为：  
 $$\nabla_{\theta} \mathcal{L}_{DFT}(\theta)=\nabla_{\theta} \mathcal{L}_{SFT}(\theta) \cdot sg\left(\frac{1}{w}\right)=\nabla_{\theta} \mathcal{L}_{SFT}(\theta) \cdot sg\left(\pi_{\theta}\left(y^{*} | x\right)\right) . (7) $$

其中，$sg(\cdot)$表示停止梯度算子，其作用是确保梯度不会流向奖励缩放项$w$。为便于后续公式推导，我们直接将$1/w$写为$\pi_{\theta}\left(y^{*} | x\right)$（而非$\pi_{\theta}(y | x)$），这是因为公式5或公式6中的指示函数会使所有$y \neq y^{*}$的情况取值为0，不影响最终结果。由于梯度不会流向缩放项，修正后的SFT损失也成为一种简单的重加权损失，即动态微调（DFT）损失，其定义为：  
 $$\mathcal{L}_{DFT}(\theta)=\mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[sg\left(\pi_{\theta}\left(y_{t}^{*} | x\right)\right) \cdot log \pi_{\theta}\left(y_{t}^{*} | x\right)\right] . (8) $$

然而在实际应用中，对整个轨迹计算重要性权重可能引发数值不稳定性。解决这一问题的常用方法是在token级应用重要性采样（正如PPO算法中所采用的策略（Schulman et al., 2017））。据此，我们得到最终的DFT损失形式：  
 $$\mathcal{L}_{DFT}(\theta)=\mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[-\sum_{t=1}^{\left|y^{*}\right|} sg\left(\pi_{\theta}\left(y_{t}^{*} | y_{<t}^{*}, x\right)\right) \cdot log \pi_{\theta}\left(y_{t}^{*} | y_{<t}^{*}, x\right)\right] . $$

需注意的是，修正后的SFT（即DFT）在RL形式下的奖励，对所有专家轨迹均统一取值为1。这与当前基于验证的奖励方法RLVR（DeepSeek-AI et al., 2025）的思路类似——后者对所有正确样本赋予统一奖励。因此，DFT无需引入额外的采样或奖励模型，即可避免模型过度聚焦于特定低概率参考token，从而实现更稳定的更新和更优的泛化能力。

# 4 实验
## 4.1 主要实验——SFT设置
我们聚焦于标准SFT场景，该场景的特点是仅拥有专家演示数据，无负样本、奖励模型或验证信号。我们使用的专家数据集通常来自外部策略（如专家模型或人工标注）。本实验的核心目标是严谨验证：在不同任务、模型架构、模型规模及数据集规模下，DFT是否能稳定优于标准SFT。

### 4.1.1 设置与实现细节
#### 数据集与模型
我们使用NuminaMath CoT数据集（LI等人，2024）进行训练，该数据集包含约86万个数学问题及其对应解答，数据来源涵盖中国高中数学练习题、美国及国际数学奥林匹克竞赛题目等。为高效利用计算资源，我们从数据集中随机抽取10万个样本用于训练——这一规模已足够，因为评估精度曲线显示，所有方法在数据集尚未完全使用时便已收敛。  
我们在多个最先进模型上开展实验，包括Qwen2.5-Math-1.5B、Qwen2.5-Math-7B（Qwen团队，2024a）、LLaMA-3.2-3B、LLaMA-3.1-8B（Dubey等人，2024）以及DeepSeekMath-7B-Base（Shao等人，2024）。

#### 训练细节
我们的实现基于verl框架（Sheng等人，2025），并采用推荐的SFT超参数。具体而言，所有模型均使用AdamW优化器，学习率设为$5 ×10^{-5}$；仅LLaMA-3.1-8B模型采用更低的学习率（$2 ×10^{-5}$）。迷你批次大小（mini-batch size）设为256，最大输入长度为2048个token。学习率采用余弦衰减调度，预热比例（warm-up ratio）为0.1。  
为进行对比，我们还纳入了同期提出的重要性加权SFT（iw-SFT，Qin & Springenberg，2025）方法。除训练轮数（epoch）设为1外，其余所有训练设置均遵循原论文报告的参数。

#### 评估设置
在数学推理任务中，我们在多个已确立的基准上进行评估，包括Math500（Hendrycks等人）、Minerva Math（Lewkowycz等人，2022）、奥林匹克基准（AI Mathematical Olympiad，2024）、2024年美国数学邀请赛（AIME 2024，American Institute of Mathematics，2024）及2023年美国数学竞赛（AMC 2023，Mathematical Association of America，2023）。  
所有模型均使用默认对话模板及思维链（Chain-of-Thought，CoT）提示，以激发逐步推理过程。报告的所有结果均为16次解码运行的平均精度，解码时温度（temperature）设为1.0，最大生成长度为4096个token。

### 4.1.2 主要结果
在所有评估的LLM上，相较于标准SFT，DFT始终能为基础模型带来显著的平均性能提升。如表1所示，以Qwen2.5-Math-1.5B为例，DFT使模型平均精度提升15.66分，这一提升幅度是SFT（+2.09分）的5.9倍以上。这种优势在其他模型家族及不同规模模型上均一致存在：  
- LLaMA-3.2-3B通过DFT获得3.46分提升，超出SFT的提升幅度（+2.05分）约1.4倍；  
- LLaMA-3.1-8B通过DFT提升10.02分，是SFT（+5.33分）的1.88倍；  
- DeepSeekMath-7B通过DFT提升15.51分，是SFT（+7.18分）的1.58倍；  
- Qwen2.5-Math-7B通过DFT提升15.90分，几乎是SFT（+2.37分）的3.8倍。

DFT展现出优异的泛化能力与稳健性，尤其在标准SFT效果微弱甚至性能下降的挑战性基准上表现突出。例如：  
- 在奥林匹克基准上，SFT导致Qwen2.5-Math-1.5B的精度从15.88降至12.63，而DFT将其提升至27.08，相对基础模型提升11.20分；  
- 在2024年美国数学邀请赛（AIME 2024）上，SFT使Qwen2.5-Math-7B的精度下降4.20分（从6.68降至2.48），而DFT即便面对该基准的高难度，仍将精度提升至8.56，相对基础模型提升1.88分；  
- 在AMC 2023上也观察到类似趋势：SFT使Qwen2.5-Math-1.5B的精度从19.38降至18.75，而DFT将其提升至38.13，相对基础模型提升18.75分；对于Qwen2.5-Math-7B，SFT仅带来1.86分的微弱提升，而DFT的提升幅度高达17.04分。  

这些结果表明，DFT不仅能在不同能力的模型上高效发挥作用，还能在传统SFT难以应对的复杂推理任务中保持稳健性，凸显其作为更可靠微调范式、提升LLM数学推理能力的潜力。

DFT还具备更优的学习效率与更快的收敛特性。图1显示，在Qwen2.5-Math-1.5B模型的所有数学推理基准上，DFT与标准SFT的学习动态存在显著差异。相较于SFT，DFT展现出三大优势：（1）收敛更快，在多数基准上仅需前120个训练步骤即可达到性能峰值；（2）早期性能更优，在训练前10-20步内，DFT的性能便已超过SFT的最佳最终精度；（3）样本效率更高，始终只需更少的更新步骤即可达到相对最优结果。这种加速收敛现象表明，DFT中的动态重加权机制能产生更具信息价值的梯度更新，在训练初期就能引导模型朝着高质量解决方案收敛。这也意味着DFT有助于规避标准SFT中常见的优化平台期或噪声敏感区域，从而更高效地让模型习得复杂的数学推理模式。

在多数模型家族及基准设置下，DFT的性能均优于同期的重要性加权SFT（iw-SFT）。如表2所示，在多数模型家族上，DFT的平均精度高于iw-SFT：LLaMA-3.2-3B（+2.39分）、LLaMA-3.1-8B（+4.15分）、DeepSeekMath-7B（+3.34分）及Qwen2.5-Math-1.5B（+1.30分）。尽管iw-SFT在Qwen2.5-Math-7B上的性能略优于DFT（+2.45分），但这种优势在不同数据集上并不稳定。尤其在LLaMA模型家族上，iw-SFT的稳健性明显不足：对于LLaMA-3.2-3B，iw-SFT在Math500（5.13分 vs 8.65分）和AMC 2023（2.03分 vs 3.13分）上的性能均低于标准SFT；对于LLaMA-3.1-8B，iw-SFT在Minerva Math（4.31分 vs 5.78分）和AMC 2023（7.34分 vs 8.28分）上的表现也逊于SFT。这些案例表明，iw-SFT可能难以泛化到特定训练信号之外，甚至在分布偏移或高难度基准上会导致性能下降。相比之下，DFT在几乎所有数据集上（包括iw-SFT表现不佳的场景）均能同时优于基础模型与SFT，凸显其在多样数学推理场景中的更优泛化能力。此外，iw-SFT需依赖额外的参考模型计算重要性权重，因此会产生额外计算开销；而DFT可直接从模型的token概率中动态获取权重，训练流程更为高效。


## 4.2 探索实验——离线RL设置
### 4.2.1 设置与实现细节
#### 数据准备
我们探索了DFT在离线RL场景中的适用性——与SFT场景相比，该场景下奖励信号的稀疏性问题可得到缓解。具体而言，我们采用了常用的拒绝采样微调（RFT）框架（Dong等人，2023；Ahn等人，2024）。参照4.1节的设置，我们从基础模型中为1万个数学问题生成响应（温度设为1.0，每个问题生成4个响应）。通过数学验证识别正确响应并将其作为训练数据，最终得到约14万个训练样本。在DPO训练中，我们从生成的响应中构建了10万个正负偏好对。

#### 训练细节
所有实验均基于Qwen2.5-Math-1.5B模型开展。我们将DFT与代表性离线RL方法（包括DPO（Rafailov等人，2023）、RFT（Dong等人，2023；Ahn等人，2024））及在线RL方法（PPO（Schulman等人，2017）、GRPO（Shao等人，2024））进行对比。  
RFT与DFT的训练设置遵循4.1节的配置；DPO训练基于ms-swift框架（Zhao等人，2024），学习率设为$1 ×10^{-6}$，批次大小为128，预热比例为0.05；PPO与GRPO训练基于verl框架（Sheng等人，2025），学习率设为$1 ×10^{-6}$，批次大小为256，预热比例为0.1，且GRPO的每个输入对应生成4个响应（$n=4$）。

### 4.2.2 结果
在离线强化学习场景中，DFT表现最佳，性能超过所有离线及在线RL基准方法。如表3所示，DFT的平均得分为35.43分，比最佳离线方法RFT（23.97分）高出11.46分，甚至超过性能最强的在线RL算法GRPO（32.00分）3.43分。DFT在所有5个基准上均表现优异：  
- 在Math500上，DFT得分为64.71分，略高于GRPO（62.86分），且优于PPO（56.10分）与RFT（48.23分）；  
- 在高难度基准上的优势更为显著：在AMC 2023上，DFT得分为48.44分，比GRPO高7.19分，比RFT高17.66分；在Minerva Math上，DFT得分为25.16分，比GRPO（18.93分）高6.23分，比PPO（15.41分）高9.75分，且与所有离线基准方法的差距更大。

我们还在离线场景下将DFT与同期的iw-SFT（Qin & Springenberg，2025）方法进行了对比。尽管iw-SFT在部分数据集上表现具有竞争力（如Math500得60.80分、AMC 2023得44.21分），但其整体平均性能（31.86分）仍比DFT低3.57分。此外，相较于其在标准SFT场景下的性能，iw-SFT在离线RL场景中的提升十分微弱——在离线RL场景中平均得分为31.86分，而在SFT场景中为30.28分，仅提升1.58分。这与DFT的提升幅度（从30.67分提升至35.43分，+4.76分）形成鲜明对比。这些结果表明，iw-SFT难以在离线约束下有效利用奖励监督信号，而DFT则能稳定地将这类信号转化为更稳健的泛化能力与更优的任务性能。

这些结果凸显了DFT作为简单高效微调策略的优势。尽管DFT无需迭代式奖励建模或环境交互，但在特定规模训练集下，其提供的学习信号强度超过了DPO/RFT等离线方法及PPO/GRPO等在线策略优化算法。这表明，在偏好监督信号可获取、但奖励建模或在线响应采样成本高或不可行的领域，DFT可作为传统RL流程的更高效、更具扩展性的替代方案。


## 4.3 消融与研究
### 4.3.1 Token概率分布
为理解DFT训练的模型与标准SFT及其他RL方法训练的模型之间的差异，我们通过图2分析了模型在训练集上输出的token概率分布。结果揭示了不同方法对概率分布的改变模式：  
SFT倾向于均匀提高所有token的概率，使整个分布向更高置信度偏移，且这种偏移主要针对低概率及极低概率token，而高概率token的占比几乎无增长。与之形成鲜明对比的是，DFT呈现出两极分化效应：它会显著提高部分token的概率，同时主动降低另一部分token的概率。这使得分布呈现双峰形态，更多token集中在最高概率区间与最低概率区间。DPO、GRPO、PPO等其他RL方法虽也呈现与DFT相似的趋势，但效应规模远弱于DFT。我们对最低概率区间的token进行分析后发现，它们多为连接词或标点符号（如“the”“let”“,”“.”等）。

这些结果表明，模型若想实现稳健学习，无需以均匀置信度拟合所有token。对于大语言模型而言，优先降低对“仅起语法功能、无核心语义”token的拟合优先级，可能更有利于学习。这一理念与人类教学逻辑相似——在教学中，学生被引导聚焦于核心概念，而非一味追求常用连接词的使用精准度。

### 4.3.2 训练超参数消融
为评估DFT方法对关键训练超参数的稳健性与敏感性，我们以Qwen2.5-Math-1.5B为基础模型，围绕学习率与批次大小开展了消融实验。该分析旨在解答两个核心问题：（1）DFT与SFT之间的性能差距，是否由SFT的超参数配置非最优导致？（2）两种方法对学习率与批次大小的变化敏感度如何？

我们在4个学习率（2e-4、1e-4、5e-5、1e-5）下对DFT与SFT进行了评估。如图3（左）所示，两种方法均对学习率存在一定敏感度，但DFT在所有配置下均稳定优于SFT——这表明两者的性能差距不能仅归因于SFT的超参数配置非最优。对两种方法而言，中等学习率（1e-4与5e-5）能带来最佳性能，而过低（1e-5）或过高（2e-4）的学习率均会导致性能显著下降。这些发现凸显了在基于梯度的微调中，合理调整学习率的重要性。

我们进一步评估了批次大小（取值范围32至256）的影响。如图3（右）所示，在整个批次大小范围内，DFT与SFT的性能均相对稳定。尽管存在微小波动，但未观察到“批次大小增大或减小会显著影响最终精度”的一致趋势。这表明，在本实验设置中，批次大小并非两种方法的主导影响因素，实际应用中采用默认批次大小即可。

# 5 结论
在本研究中，我们针对监督微调（SFT）与强化学习（RL）之间已有充分研究证实的泛化差距展开研究。我们提出了一项创新性理论分析，证实标准SFT的梯度等价于一种策略梯度更新——该策略梯度具有不适定的隐含奖励，且奖励大小与模型的置信度成反比。这一洞见解释了SFT为何容易过拟合，以及其优化动态为何不稳定。基于该分析，我们提出了动态微调（DFT）方法：这是一种简单却高效的策略，通过利用token概率对SFT损失进行动态重加权，从而解决上述问题。仅需一行代码的修改，DFT便能稳定学习过程，并促进模型实现更优泛化。

我们的大量实验表明，在各类模型及具有挑战性的数学推理基准任务上，DFT始终能显著优于标准SFT。此外，当将DFT适配到离线RL场景时，它还出人意料地超越了已有的在线及离线RL算法，充分彰显了其有效性与高效性。本研究不仅深化了对SFT的理解，还提供了一种具有实际应用价值且影响显著的解决方案，有效缩小了SFT与更复杂RL方法之间的性能差距。

**局限性**：尽管我们的实验证实DFT在数学推理基准上能带来显著性能提升，但该评估仍局限于数学推理领域的数据集，以及参数规模不超过70亿的模型。目前，我们尚未在其他任务领域（如代码生成、常识问答）或更大规模的大语言模型（如130亿参数及以上模型）上评估DFT的性能。此外，当前研究也仅局限于纯文本场景。在未来工作中，我们不仅计划将研究扩展到更广泛的文本基准任务、将DFT应用于最先进的大模型以验证其扩展性，还将在视觉-语言任务上验证其有效性，以确认DFT在多模态场景下的通用性。