# SFT 与 DPO

# 重要公式
## 目标函数
$$
\min _{\pi} D_{f}\left(\mu_{\pi} \| \mu_{E}\right) \underbrace{+\beta D_{KL}\left(\pi \| \pi_{r e f}\right)}_{传统设置中的-\beta \mathcal{H}(\pi) }
$$

$$
\max _{\pi} \mathbb{E}_{\mu_{E}}\left[\beta \underbrace{\log \pi(y | x)}_{极大似然估计（MLE）}-\underbrace{\log \pi_{ref }(y | x)-V_{\pi}\left(s_{t}\right)}_{作为常数项}\right]
$$

# 结论

## 结论1
常用的SFT是寻找隐式奖励的一种特殊情况，与DPO相同。我们的推导表明，当选择总变差作为f散度测度时，常用的SFT损失本质上是模仿学习中“奖励发现”的一种特殊情况（不同f散度衍生的替代训练目标见附录C）。

## 结论2
常用SFT中KL项缺失。公式（5）中“基础模型与策略模型的差异项”呈零阶形式，在随机梯度下降过程中相当于常数项，无法对策略模型的更新步长形成约束——而该约束在大多数强化学习算法中具有重要作用。KL项的缺失导致DPO后模型（post-DPO model）的训练起始点与基础模型之间存在较大偏差。为此，我们提出一种简单有效的改进方法：通过降低学习率来减小优化步长。

4.2节的实验表明，该方法在指令跟随任务中显著提升了模型性能。此外，通过选择不同的f散度，可推导出与SFT类似但能保留KL项的目标函数。不过，多数这类目标函数涉及对数或指数运算，可能导致数值不稳定。我们选择了三种具有代表性的散度，并在4.3节中展示了它们的对比结果。

## 实验
### 模型与数据集选择
指令跟随能力是大型语言模型（LLM）完成多数下游任务的核心基础。参考SimPO[25]的实验设置，本研究选用Llama3-8B[2]与Mistral-7B[5]作为基础模型；SFT数据集采用常用的UltraChat-200K[30]；在通用指令跟随任务中，模型通常先在UltraChat-200K上完成SFT训练，再在Ultra-feedback[31]数据集上进行DPO训练以得到最终模型，本研究的实验亦采用这两个数据集。


### 超参数、设备、基线模型与评估基准
后训练中最常用的学习率为：SFT阶段2e-5，DPO阶段5e-7。所有实验均采用128的批次大小（batch size），在8台H100 GPU上基于OpenRLHF[32]框架完成训练。在DPO训练过程中，β是关键超参数，本研究设置β=0.01。

模型评估采用AlpacaEval2[33]、Arena Hard[34]与MT-bench[35]三个基准。由于这些基准的结果易受实现细节（如vLLM[36]版本）影响，我们与SimPO保持了一致的实现版本；下游评估时也采用了与SimPO相同的解码参数。考虑到同时在三个基准上评估会产生较高的API成本，部分实验仅以AlpacaEval2作为代表性基准。

### 4.2 SFT采用小学习率可提升post-DPO模型性能
结果如表1所示（表中蓝色标记的模型表示SFT阶段采用小学习率，且后续基于该SFT checkpoint进行微调的模型）。我们复现的基线结果优于SimPO原论文报告的结果，且所有实验设置保持一致以确保公平对比。

从结果可见，降低学习率不仅能小幅提升SFT checkpoint的性能，还能显著增强后续对齐算法（DPO/SimPO）优化后的模型效果：在SimPO结果中，Llama3-8B的相对提升达20%（绝对提升5%），Mistral的相对提升达25%（绝对提升6%）。由于DPO训练过程的超参数保持一致，性能提升主要源于SFT阶段学习率的调整，这验证了我们的理论假设。


### 4.3 其他模仿损失形式在序贯训练中表现更优
结果如表3所示（表中SFT指由总变差散度推导的常用训练目标，Pearson-SFT指由皮尔逊χ²散度推导的模仿目标，SH-SFT指由平方海林格散度推导的模仿目标）。可以发现：无论基于Mistral还是Llama模型，皮尔逊χ²散度与平方海林格散度均导致SFT阶段性能较弱，但DPO后性能更优。这一结果引出一个有趣结论：**更优的SFT checkpoint未必能带来更优的DPO结果**。

基于KL正则化SFT的性能提升验证了我们的理论——即后训练过程中KL散度项的重要性。三种SFT方法的训练损失曲线如图2c所示。


### 4.4 SFT中的价值与奖励
基于3.3节的理论结论，我们利用LLM的logits估算价值函数。传统价值估算通常依赖蒙特卡洛采样，且价值计算需基于真实奖励——但本研究中的奖励是隐式的，无法直接获取。因此，我们旨在通过实证证明：LLM的logits具备价值函数属性，其得分可用于评估状态质量。

具体而言，我们验证了：在同一领域训练的LLM，对不同状态的评估排序在不同模型间具有一致性。实验设计如下：
1. 将UltraChat-200K数据集拆分为4个分片，在Llama-3基础模型上分别进行SFT训练，得到4个不同的checkpoint；
2. 选择Llama-3-instruct（与上述模型具有相同先验，但训练数据集不同）与Zephyr[41]（先验不同，但训练数据集相似）作为对比模型；
3. 选用MATH-500[42]作为验证集，为每个问题采样一条轨迹，并将推理过程拆分为多个步骤；
4. 输入每个步骤的最终令牌后，提取模型输出的logits，计算其对数求和指数（log-sum-exp），并在每个模型内部对这些值进行排序；
5. 计算不同模型间排序结果的肯德尔等级相关系数（KLCC），以衡量排序一致性。

#### 主要结果
结果如图2a所示。所有实验设置下，LLM的价值排序均呈正相关：4个数据集分片对应的模型排序相关系数接近1；即使后训练数据集或模型先验发生变化，相关性仍保持为正。一个有趣的发现是：尽管Zephyr基于UltraChat训练（与我们的分片模型数据集相同），但其与Llama3-instruct的排序相关性显著高于与分片模型的相关性。这种正相关性间接验证了LLM的logits具备价值评估属性，从而支持了假设2。

#### 关于隐式奖励的另一发现
我们发现：模型隐式奖励与下游任务奖励的对齐程度越高，模型性能越优。在SFT训练过程中，我们使用AlpacaEval2已标注的成对数据（作为评估集）计算DPO损失——由于该基准可视为“环境”，DPO损失越低，表明隐式奖励与环境奖励的一致性越高。结果如图2b所示，奖励与环境的对齐程度与模型性能呈正相关，这与直觉预期一致。


### 4.5 通过SFT实现奖励稳定
根据我们的理论，SFT的作用是将隐式奖励调整至合理范围，以便后续进行精细化修正。假设1认为：在隐式奖励搜索的起始点，模型是某一未知奖励函数下的最优策略——但该奖励函数可能与真实下游任务的奖励函数存在显著差异。

为验证这一假设，我们绘制了SFT过程中“提示词后第一个logits的对数求和指数”（即V(s₀)）的训练曲线，并生成了多个SFT早停（early-exit）checkpoint，在这些checkpoint上进行DPO训练。结果如图3所示：V(s₀)在SFT过程中快速上升并迅速收敛，且下游任务性能的变化趋势与V(s₀)高度一致。由此可得出结论：SFT在训练150步时已完成“将隐式奖励调整至合理范围”的任务，后续步骤主要聚焦于更精细的建模。


# 摘要
后训练过程是将预训练语言模型适配到真实世界任务中的关键阶段，而从演示样本或偏好信号中学习在这一适配过程中发挥着重要作用。本文提出了一个统一的理论框架，用于衔接大型语言模型（LLM）后训练中的有监督微调（SFT）与偏好学习。通过严谨的数学推导，我们证明：SFT与直接偏好优化（DPO）等偏好学习方法均在同一最优策略-奖励子空间内运行，且SFT是隐式奖励学习的一种特殊情况。

我们的分析揭示了传统SFT存在的一个关键局限：在优化过程中，分布匹配中的KL散度项相对于策略为常数，无法对模型更新形成约束。为解决这一问题，我们提出了一种简单且有效的学习率降低方法，该方法能显著提升模型性能（在指令跟随任务中，相对增益高达25%，绝对胜率提升高达6%）。此外，我们还从多种f散度函数中推导出了替代SFT目标，这些目标在优化过程中能保留KL散度项，进而进一步提升DPO后模型（post-DPO model）的性能。最后，我们将偏好学习中LLM对数概率（logits）与Q函数的理论关系扩展到SFT场景，并提供了数学推导与实验验证。

# 1 引言
后训练是将大型语言模型（LLM）适配到实际应用中的关键阶段。在从大量预训练语料库中积累通用先验知识后，后训练旨在挖掘LLM的潜力以满足不同需求，例如遵循自然语言指令[1,2,3,4,5]。

后训练领域主要由两种方法主导。第一种方法从专家演示中学习[6,7]，通常被称为模仿学习，在LLM场景下一般称为有监督微调（SFT）。第二种方法侧重于从环境信号中学习，主要通过强化学习方法实现[8,9,10]。

在后训练领域中，偏好信号已成为一种极具价值的反馈形式，受到了广泛的研究关注[11,1]。偏好学习通常遵循两阶段流程（以下简称序贯训练）：先进行SFT，再采用直接偏好优化（DPO）等偏好优化方法[12]。然而，人们对这两个关键阶段之间关系的理解仍主要基于经验观察，而非理论基础，且SFT常被仅视为预备性的热身步骤[13]。

尽管这种序贯范式被广泛采用，但在理论层面，关于这两种方法如何从根本上相互关联，仍存在显著缺口。尽管先前的研究[14]已深入探索了LLM学习动态的各个方面，但SFT目标与偏好学习框架之间的理论关联尚未得到充分关注，这限制了我们对二者在后训练过程中协同有效性的理解。

为填补这一缺口，我们证明了隐式奖励学习可作为连接SFT与偏好学习过程的统一视角。先前的研究[12]已证实，第二阶段的偏好学习可通过隐式奖励实现。在本研究中，我们重新审视了分布匹配目标，并针对后训练进行了必要调整。我们基于先前研究[15,16]提供了全面的数学证明。该证明表明，传统SFT目标是隐式奖励学习的一种特殊情况。图4展示了我们的理论结论：每个可能奖励函数对应的最优策略构成一个策略-奖励子空间，SFT与DPO均在该子空间内运行。

图1：示意图：SFT与DPO在最优策略子空间中优化隐式奖励。x轴代表所有可能的策略模型空间，y轴代表所有可能的奖励函数空间。对于特定奖励函数，每个可能的策略都有其对应的期望回报，在策略-回报平面上形成一条曲线。不同奖励函数对应的最优策略构成一个子空间。左图：SFT过程在该子空间内搜索，并沿演示样本指示的平均方向移动；右图：同理，DPO也在同一子空间内运行，但沿优选样本与拒绝样本构成的方向向量搜索。

这一隐式奖励理论框架带来了多项新见解。将SFT视为从分布匹配衍生的训练目标后，我们发现一个关键问题：目标函数中至关重要的KL散度项仅作为零阶分量存在。由于该术语相对于策略π为常数，在求导后无法对模型更新形成约束。我们提出了一种简单且高效的策略，通过降低学习率来缓解这一问题。此外，我们通过为分布匹配选择不同的f-散度导数函数，确定了替代训练目标，这些目标在优化过程中能保留KL散度项，并通过实验证明了其有效性。最后，我们证明了在SFT过程中，LLM的对数概率（logits）可作为与隐式奖励对应的Q函数。这扩展了[17]中的理论框架——该研究主要在DPO场景下建立了这一关系，而我们的工作则在SFT过程中发现了类似的数学结构。在指令跟随任务中，我们的实验训练结果与这些理论预测高度一致。


# 2 相关工作
## 2.1 逆强化学习
逆强化学习由[18]首次正式提出，其核心假设是：专家策略是在某一奖励函数下的最优策略。相关详细介绍见附录A。

与[15,16]采用贝尔曼方程的方法不同，本文采用闭解（closed-form solutions），避免了内循环优化（inner-loop optimization）。与[19]相比，本文的研究采用了更通用的f散度（f-divergence）公式[20]，且未引入负样本。此外，不同于[21]，本文的研究重点是在大型语言模型（LLM）的序贯训练场景下，探究有监督微调（SFT）与直接偏好优化（DPO）之间的关系。

## 2.2 大型语言模型中的隐式奖励
传统的基于人类反馈的强化学习（RLHF）方法通常需要大量计算资源（例如，PPO[22]在训练过程中需使用4个模型），而直接偏好优化（DPO）[12]的提出正是为了降低强化学习过程的计算开销。研究证实，DPO的最大化问题存在闭解，这使得我们可以直接基于隐式奖励进行奖励建模。此后，众多相关研究（如IPO[23]、KTO[24]、SimPO[25]、R-DPO[26]）均沿着这一方向展开。

此外，[17]还在DPO场景下建立了Q函数（Q-functions）与LLM对数概率（logits）之间的关系；[27]则将logits视为Q值（Q-values），并在机器人任务中训练了一个Q头（Q-head）。而本文的研究证明，在SFT过程中也存在类似的（logits与Q函数的）关联结构。

## 2.3 大型语言模型的后训练理论分析
目前已有部分研究对LLM后训练阶段的各类关系展开分析。例如，[28]分析了隐式奖励与奖励模型之间的同构关系（isomorphic relationship），并将隐式奖励与“生成-验证差距”（generation-verification gap）相关联；[14]则更关注不同训练阶段的学习动态（learning dynamics）。

与之不同的是，本文的研究更聚焦于后训练过程中各训练目标之间的关系，并以隐式奖励为媒介来理解这些关系。

# 3 SFT与DPO的统一视角
## 3.1 LLM中的令牌级马尔可夫决策过程（MDP）
在语言模型中，马尔可夫决策过程（MDP）适用于令牌级别的决策。其中，状态$s \in S$表示上下文，动作$a \in A$为可能的下一个令牌，策略$P(a | s)$给出令牌的概率分布。状态转移$T(s' | s, a)$是确定性的，满足$s'=s \oplus a$（即令牌附加到上下文后形成的新状态）。模型会为每个动作选择获得奖励$r(s, a)$，这一过程持续进行，直至达到终止状态。$\gamma$为折扣因子。该框架将语言模型在文本生成过程中的序贯决策行为形式化。

## 3.2 定义1（占据测度[15]）
策略$\pi$的占据测度定义为：
$$
\rho(s)=(1-\gamma) \sum_{i=0}^{\infty} \gamma^{i} P\left(s_{i}=s | \pi\right)
$$
其中，$P(s_{i}=s | \pi)$表示在策略$\pi$下，第$i$步时刻访问到状态$s$的概率。

## 3.3 定义2（状态-动作分布[15]）
策略$\pi$的状态-动作分布定义为：
$$
\mu (s,a)=\pi (a|s)\rho (s)
$$
其中，$\mu(s, a)$表示由策略$\pi$诱导产生的、在状态-动作对上的平稳分布。

### 3.1 后训练中的分布匹配
分布匹配是模仿学习中一种成熟的训练目标[29, 15]，其核心是最小化专家的状态-动作分布$\mu_E$与策略模型的状态-动作分布$\mu_\pi$之间的f散度，同时引入熵正则项以促进探索。然而，在LLM的后训练场景中，熵正则项并不完全适用——对整个词汇表进行概率平均可能会破坏预训练阶段为基础模型建立的自然语言先验。因此，我们将正则项从熵替换为基础模型$\pi_{ref}$与策略模型$\pi$之间的KL散度（Kullback-Leibler散度），目标公式如下：
$$
\min _{\pi} D_{f}\left(\mu_{\pi} \| \mu_{E}\right) \underbrace{+\beta D_{KL}\left(\pi \| \pi_{r e f}\right)}_{传统设置中的-\beta \mathcal{H}(\pi) }
$$
其中，$D_f(\cdot \| \cdot)$表示f散度，$\mathcal{H}(\pi)$为策略的熵；$\beta$是正则项系数，通常作为超参数使用。

[28]中也提出了类似的目标函数。本文从不同的理论视角对这一概念进行探讨，并补充说明如下：
1. **从交叉熵项视角**：KL散度可分解为熵项与交叉熵项之和，即$\mathcal{H}(\pi, \pi_{ref}) - \mathcal{H}(\pi)$。由于基础模型在预训练阶段已收敛，且接触过海量自然语言数据，其交叉熵项不应过大。这意味着，策略模型在最大化探索时，不应偏离自然语言的范畴。
2. **从KL散度视角**：通过预训练得到的基础模型已具备足够的质量，且包含丰富的额外知识。因此，在为实现分布匹配而最小化散度时，需保留基础模型的内在属性。

### 3.2 模仿学习作为隐式奖励发现
借鉴非对抗性模仿学习[15]的推导过程，可将训练目标转化为等价的最小-最大（min-max）问题。由此得到以下关键结论：

#### 定理1（分布匹配的等价目标）
学习一个能最小化专家与策略状态-动作分布之间f散度的策略，等价于先学习某个任意奖励函数下的最优策略，再对该奖励函数的某一函数进行优化，具体公式如下：
$$
-\min _{r}\left[\mathbb{E}_{\mu_{E}}\left[f^{*}(-r)\right]+\underbrace{\max _{\pi} \mathbb{E}_{\mu_{\pi}}[r]-\beta D_{KL}\left(\pi \| \pi_{ref }\right)}_{存在闭解}\right]
$$
其中，$f^*$是所选f散度对应的凸共轭函数，$\mu_\pi$是待学习策略的状态-动作分布，$\mu_E$是专家的状态-动作分布；$r$是$f^*$的自变量，通常被解释为奖励函数。

详细证明见附录B.1。此处的奖励函数$r$尚未与隐式奖励关联，仅代表一个任意函数。常用的SFT损失与分布匹配方法在本质上仍存在差异，但该公式建立了“寻找策略模型”与“确定合适奖励函数”之间的联系。尽管该等价目标被表述为双层优化问题，但其中后一部分（记为$J(\pi)=\mathbb{E}_{\mu_{\pi}}[r]-\beta D_{KL}(\pi \| \pi_{ref})$）的闭解已由[17]证明存在。我们在研究中借鉴了这一已验证的结论，相关公式如下：
$$
\pi ^{*}=\arg \max_{\pi }J(\pi ),\quad J(\pi ^{*})=V^{*}(s_{0})
$$
其中，$\pi^*(a | s)$为最优策略，$V^*$为最优策略的价值函数。

#### 引理1（奖励与策略的关系[17]：隐式奖励）
奖励与对应最优策略之间的关系为：
$$
r(x, y)=\beta \log \frac{\pi^{*}(y | x)}{\pi_{r e f}(y | x)}+V^{*}\left(s_{0}\right)-V^{*}\left(s_{t}\right)
$$
其中，$r(x, y)$表示LLM在输入-输出对$(x, y)$上的奖励。

由于训练目标公式（2）中的奖励$r$可任意初始化，我们提出以下假设：

#### 假设1（初始奖励简化）
不失一般性，可将初始奖励$r$视为$V_{\pi}(s_0)$，且对所有$t>0$，满足$V_{\pi}(s_t)=0$。在此假设下，初始策略$\pi$是相对于初始奖励的最优策略。

此时，可将公式（2）的后一部分替换为其闭解。已有研究[20]已建立了散度测度与其共轭函数之间的对应关系，相关内容也列于附录C中。本研究选择总变差距离作为散度测度，其对应的共轭函数为恒等函数。此外，奖励与策略$\pi$的关系满足公式（4），由此可直接得到最终目标——该目标与常用的SFT形式一致：
$$
\max _{\pi} \mathbb{E}_{\mu_{E}}\left[\beta \underbrace{\log \pi(y | x)}_{极大似然估计（MLE）}-\underbrace{\log \pi_{ref }(y | x)-V_{\pi}\left(s_{t}\right)}_{作为常数项}\right]
$$
其中，$s_t$表示终止状态，该状态后的期望回报为常数。

#### 结论1
常用的SFT是寻找隐式奖励的一种特殊情况，与DPO相同。我们的推导表明，当选择总变差作为f散度测度时，常用的SFT损失本质上是模仿学习中“奖励发现”的一种特殊情况（不同f散度衍生的替代训练目标见附录C）。由于上述推导过程可逆，可得出结论：SFT过程在最优策略-奖励子空间内搜索，试图对专家演示中隐含的奖励进行建模。在SFT过程的起始点，策略是假设1下的最优策略；在优化过程中，模型与奖励的关系始终满足公式（4），从而实现对策略-奖励最优子空间的搜索。这种隐式奖励结构与DPO中的隐式奖励结构完全一致，为两种方法提供了统一的理论视角。

#### 结论2
常用SFT中KL项缺失。公式（5）中“基础模型与策略模型的差异项”呈零阶形式，在随机梯度下降过程中相当于常数项，无法对策略模型的更新步长形成约束——而该约束在大多数强化学习算法中具有重要作用。KL项的缺失导致DPO后模型（post-DPO model）的训练起始点与基础模型之间存在较大偏差。为此，我们提出一种简单有效的改进方法：通过降低学习率来减小优化步长。4.2节的实验表明，该方法在指令跟随任务中显著提升了模型性能。此外，通过选择不同的f散度，可推导出与SFT类似但能保留KL项的目标函数。不过，多数这类目标函数涉及对数或指数运算，可能导致数值不稳定。我们选择了三种具有代表性的散度，并在4.3节中展示了它们的对比结果。

### 3.3 SFT过程中模型保持内在期望回报估计
在DPO过程中，[17]指出，在温和假设下，LLM的对数概率（logits）可被解释为Q函数。基于公式（2）中的类似结构，我们将这一结论进行扩展：

#### 定理2（内在期望回报）
在SFT过程中，语言模型的对数概率$l_a$对应于所学习隐式奖励的Q函数$Q(s, a)$，具体关系如下：
$$
l_{a}=Q_{\hat{r}}(s, a)+C(s)=\hat{r}(s, a)+\gamma \mathbb{E}_{s_{t+1} \sim P(\cdot | s, a)}\left[V_{\hat{r}}\left(s_{t+1}\right)\right] \tag{6}
$$
其中，$\hat{r}$是模型满足公式（4）的隐式奖励函数，$\gamma$为折扣因子，$V_{\hat{r}}(s_{t+1})$是下一状态的价值函数；$C(s)$是仅依赖于当前状态的函数。

详细证明见附录B.3。研究结果表明，不仅在DPO过程中，在SFT过程中，模型的对数概率同样可被解释为Q函数——该Q函数表征了模型基于自身学习到的隐式奖励对“期望回报”的估计。函数$C(s)$代表真实Q值与对数概率之间的差距，但由于它仅依赖于当前状态，且对所有动作而言均为常数，因此不会影响不同动作之间的相对排序。根据附录B的内容，价值函数可通过log-sum-exp（对数求和指数）计算得到，据此我们提出以下假设：

#### 假设2（价值主导假设）
对于绝大多数状态$s_1$和$s_2$，$C(s_1)$与$C(s_2)$的差值小于$V(s_1)$与$V(s_2)$的差值。

该结论使得我们在使用其他散度形式时，可利用LLM对数概率的log-sum-exp来计算价值函数，而非采用蒙特卡洛采样。

# 4 实证研究
本节针对指令跟随任务展开实证分析，具体内容如下：

首先，SFT阶段采用小学习率可显著提升后续DPO后模型（post-DPO model）的性能。正如3.2节所述，我们通过理论证明：在序贯训练中，降低SFT阶段的学习率以减小单步优化步长，可有效改善模型效果，相关验证将在4.2节展开。

其次，能保留KL散度项的其他f散度形式也能带来更优结果。由其他f散度推导的训练目标不会出现KL散度项缺失的问题。我们选择了皮尔逊χ²散度（Pearson χ²）与平方海林格散度（Squared Hellinger）——这两种散度可避免对数与指数运算带来的数值不稳定问题，并在4.3节验证了其性能提升效果。

第三，LLM的对数概率（logits）具备价值函数属性，对状态质量的评估方式具有一致性。借助价值函数可反映状态期望质量的特性，我们在4.4节证明：不同模型对不同状态的评估排序具有相似性。

最后，SFT可缓解初始奖励的随机性，快速将隐式奖励调整至合理范围。我们将SFT在后训练中的作用解释为修正假设1中的初始奖励，并在4.5节通过实证表明：SFT过程中V(s₀)（初始状态价值）会快速收敛。


## 4.1 基础实验设置
### 模型与数据集选择
指令跟随能力是大型语言模型（LLM）完成多数下游任务的核心基础。参考SimPO[25]的实验设置，本研究选用Llama3-8B[2]与Mistral-7B[5]作为基础模型；SFT数据集采用常用的UltraChat-200K[30]；在通用指令跟随任务中，模型通常先在UltraChat-200K上完成SFT训练，再在Ultra-feedback[31]数据集上进行DPO训练以得到最终模型，本研究的实验亦采用这两个数据集。

### 超参数、设备、基线模型与评估基准
后训练中最常用的学习率为：SFT阶段2e-5，DPO阶段5e-7。所有实验均采用128的批次大小（batch size），在8台H100 GPU上基于OpenRLHF[32]框架完成训练。在DPO训练过程中，β是关键超参数，本研究设置β=0.01。

模型评估采用AlpacaEval2[33]、Arena Hard[34]与MT-bench[35]三个基准。由于这些基准的结果易受实现细节（如vLLM[36]版本）影响，我们与SimPO保持了一致的实现版本；下游评估时也采用了与SimPO相同的解码参数。考虑到同时在三个基准上评估会产生较高的API成本，部分实验仅以AlpacaEval2作为代表性基准。


## 4.2 SFT采用小学习率可提升post-DPO模型性能
如3.2节所述，约束SFT学习过程的KL散度项（即log πₐᵉᶠ）是零阶项，求导后无法为策略优化提供梯度贡献。结合传统强化学习中步长约束的重要性，我们推测：需降低SFT阶段的学习率以减小有效更新幅度。

本研究将SFT阶段的学习率从常用的2×10⁻⁵调整为：Llama3基础模型5×10⁻⁶，Mistral基础模型1×10⁻⁶。SFT后的强化学习过程选用DPO与SimPO算法，且保持与2×10⁻⁵学习率配置相同的超参数。基线结果通过复现SimPO原始实现的公开checkpoint获得，并在相同测试环境中完成评估。

### 主要结果
结果如表1所示（表中蓝色标记的模型表示SFT阶段采用小学习率，且后续基于该SFT checkpoint进行微调的模型）。我们复现的基线结果优于SimPO原论文报告的结果，且所有实验设置保持一致以确保公平对比。

从结果可见，降低学习率不仅能小幅提升SFT checkpoint的性能，还能显著增强后续对齐算法（DPO/SimPO）优化后的模型效果：在SimPO结果中，Llama3-8B的相对提升达20%（绝对提升5%），Mistral的相对提升达25%（绝对提升6%）。由于DPO训练过程的超参数保持一致，性能提升主要源于SFT阶段学习率的调整，这验证了我们的理论假设。


## 4.3 其他模仿损失形式在序贯训练中表现更优
除总变差散度外，附录C中列出的其他f散度函数也能推导得到训练目标——这些目标中的KL散度项不再局限于零阶近似。但多数替代散度（如詹森-香农散度）涉及对数、指数运算，甚至复合log-exp运算，易导致数值不稳定。因此，本研究选择了另外两种f散度形式：皮尔逊χ²散度与平方海林格散度，并在表2中展示了其推导得到的训练目标。

其中，皮尔逊χ²散度包含平方概率差项，可起到KL约束的作用；平方海林格散度在应用链式法则后，会在经典梯度项前乘以一个与概率差相关的系数项，该系数项可调节更新步长。我们将这两种散度的结果与此前总变差散度的结果进行了对比。

### 主要结果
结果如表3所示（表中SFT指由总变差散度推导的常用训练目标，Pearson-SFT指由皮尔逊χ²散度推导的模仿目标，SH-SFT指由平方海林格散度推导的模仿目标）。可以发现：无论基于Mistral还是Llama模型，皮尔逊χ²散度与平方海林格散度均导致SFT阶段性能较弱，但DPO后性能更优。这一结果引出一个有趣结论：更优的SFT checkpoint未必能带来更优的DPO结果。

基于KL正则化SFT的性能提升验证了我们的理论——即后训练过程中KL散度项的重要性。三种SFT方法的训练损失曲线如图2c所示。


## 4.4 SFT中的价值与奖励
基于3.3节的理论结论，我们利用LLM的logits估算价值函数。传统价值估算通常依赖蒙特卡洛采样，且价值计算需基于真实奖励——但本研究中的奖励是隐式的，无法直接获取。因此，我们旨在通过实证证明：LLM的logits具备价值函数属性，其得分可用于评估状态质量。

具体而言，我们验证了：在同一领域训练的LLM，对不同状态的评估排序在不同模型间具有一致性。实验设计如下：
1. 将UltraChat-200K数据集拆分为4个分片，在Llama-3基础模型上分别进行SFT训练，得到4个不同的checkpoint；
2. 选择Llama-3-instruct（与上述模型具有相同先验，但训练数据集不同）与Zephyr[41]（先验不同，但训练数据集相似）作为对比模型；
3. 选用MATH-500[42]作为验证集，为每个问题采样一条轨迹，并将推理过程拆分为多个步骤；
4. 输入每个步骤的最终令牌后，提取模型输出的logits，计算其对数求和指数（log-sum-exp），并在每个模型内部对这些值进行排序；
5. 计算不同模型间排序结果的肯德尔等级相关系数（KLCC），以衡量排序一致性。

### 主要结果
结果如图2a所示。所有实验设置下，LLM的价值排序均呈正相关：4个数据集分片对应的模型排序相关系数接近1；即使后训练数据集或模型先验发生变化，相关性仍保持为正。一个有趣的发现是：尽管Zephyr基于UltraChat训练（与我们的分片模型数据集相同），但其与Llama3-instruct的排序相关性显著高于与分片模型的相关性。这种正相关性间接验证了LLM的logits具备价值评估属性，从而支持了假设2。

### 关于隐式奖励的另一发现
我们发现：模型隐式奖励与下游任务奖励的对齐程度越高，模型性能越优。在SFT训练过程中，我们使用AlpacaEval2已标注的成对数据（作为评估集）计算DPO损失——由于该基准可视为“环境”，DPO损失越低，表明隐式奖励与环境奖励的一致性越高。结果如图2b所示，奖励与环境的对齐程度与模型性能呈正相关，这与直觉预期一致。


## 4.5 通过SFT实现奖励稳定
根据我们的理论，SFT的作用是将隐式奖励调整至合理范围，以便后续进行精细化修正。假设1认为：在隐式奖励搜索的起始点，模型是某一未知奖励函数下的最优策略——但该奖励函数可能与真实下游任务的奖励函数存在显著差异。

为验证这一假设，我们绘制了SFT过程中“提示词后第一个logits的对数求和指数”（即V(s₀)）的训练曲线，并生成了多个SFT早停（early-exit）checkpoint，在这些checkpoint上进行DPO训练。结果如图3所示：V(s₀)在SFT过程中快速上升并迅速收敛，且下游任务性能的变化趋势与V(s₀)高度一致。由此可得出结论：SFT在训练150步时已完成“将隐式奖励调整至合理范围”的任务，后续步骤主要聚焦于更精细的建模。

# 5 更广泛影响、局限性与未来工作
## 5.1 更广泛影响：哲学层面
如前文所述，模型在SFT过程中会学习隐式奖励，这一现象可能引发进一步讨论：大型语言模型（LLM）是否可被视为具有预设环境感知能力的实体。这一问题为探索“意识本质”以及“人工系统在多大程度上可能表现出类意识属性”开辟了哲学研究方向。

## 5.2 局限性
本研究尚未对额外的散度函数展开实验探索。常用的KL散度与詹森-香农（JS）散度涉及对数与指数运算，易导致数值不稳定性。我们尝试了多种实现方式，并对数据进行了小值裁剪（small-value clipping）——尽管这一操作避免了非数字（NaN）错误，但损失值仍会达到极大值（如6e7）。由于我们未设计专门的算子来实现这些散度方法，因此KL散度等方法的有效性目前尚不明确。

## 5.3 未来工作：SFT与DPO多目标学习
鉴于SFT与DPO过程均会对隐式奖励进行建模，一个自然的思路是将二者构建为多目标学习任务，而非采用现有的序贯训练模式。附录D详细记录了我们在实现这一多目标方法时的若干失败尝试，希望这些发现能为该领域的未来研究提供参考。

# 6 结论
综上所述，本研究以**隐式奖励学习**为核心，建立了大型语言模型（LLM）后训练过程中有监督微调（SFT）与偏好学习的统一视角。我们通过推导证明：采用总变差散度时，传统SFT是隐式奖励学习的一种特殊情况，但其存在KL散度项缺失的局限。

为解决这一局限，我们提出的**降低学习率**方法显著提升了模型性能；同时，从其他f-散度推导得到的、能保留KL散度项的替代SFT目标，进一步带来了性能增益。此外，我们将DPO场景下“对数概率（logits）与Q函数的映射关系”扩展到SFT场景，并证实SFT在稳定随机隐式奖励中发挥着关键作用。这些研究成果不仅深化了对LLM后训练机制的理论理解，也为设计更高效的后训练策略提供了实践指导。


# 附录A 相关工作
## A.1 逆强化学习与f-散度
逆强化学习（Inverse Reinforcement Learning, IRL）由[18]首次正式提出，其核心假设是：**专家策略是某一奖励函数下的最优策略**。该方法旨在恢复这一奖励函数，并训练出能最大化该奖励的策略模型。[29]提出的“最大熵原理”推动了IRL的重大发展——这一原理可确保策略具备合理的探索能力，成为IRL领域的重要突破。

IRL的常用方法包括博弈论框架下的算法，其中最具代表性的是生成式对抗模仿学习（Generative Adversarial Imitation Learning, GAIL）[43]。GAIL采用对抗训练方式，先显式恢复奖励函数，再基于恢复的奖励函数学习策略。[20]从数学层面提出了基于f-散度的统一视角，为各类IRL方法提供了理论支撑；[44]同样利用不同形式的f-散度推导模仿学习方法，该方法涉及迭代优化，主要应用于经典强化学习场景。

显式学习奖励函数通常需要额外参数，且面临高方差的对抗优化问题。为规避这一局限，[15]提出学习由“奖励与策略”共同参数化的Q函数，从而避免显式学习奖励；[16]在该研究基础上，为大型语言模型提出了非对抗性模仿学习方法。[19]则采用“逆KL散度+熵正则”构建模仿学习目标，同样无需显式训练奖励函数，有助于减少过拟合。[21]通过设计不同形式的f-散度，解决了生成过程中的分布外（Out-of-Distribution, OOD）问题，并实现了序列生成中的回溯机制。

本研究对“最大熵分布匹配”的原始训练目标进行了修改，使其更适配LLM后训练场景：我们采用闭解（closed-form solution）而非[15, 16]所用的贝尔曼方程，避免了内循环优化；与[19]相比，本研究采用更通用的f-散度公式[20]，且未引入负样本。


# 附录B 理论证明
本节将详细证明本研究提出的各项理论结论。

## B.0 引理2（最大熵强化学习的不动点解[17]）
在最大熵框架下，最优策略$\pi^*(a_t | s_t)$及其对应的最优价值函数$V^*(s_t)$满足以下不动点方程：
$$
\pi^*(a_t | s_t) = \exp\left( \frac{Q^*(s_t, a_t) - V^*(s_t)}{\beta} \right) \tag{7}
$$
$$
V^*(s_t) = \beta \log \int_{\mathcal{A}} \exp\left( \frac{Q^*(s_t, a_t)}{\beta} \right) da_t
$$
$$
J(\pi^*) = V^*(s_0) \tag{9}
$$
其中，$\pi^*(a | s)$为最优策略，$Q^*$和$V^*$分别为最优策略的动作价值函数（质量函数）与状态价值函数。


## B.1 非对抗性模仿学习：奖励发现
### 定理3（分布匹配的等价目标）
学习“最小化专家与策略状态-动作分布之间f-散度”的策略，等价于“先学习某一任意奖励函数下的最优策略，再优化该奖励函数的某一函数”，具体公式如下：
$$
\min_{\pi} D_f(\mu_{\pi} \| \mu_E) + \beta D_{KL}(\pi \| \pi_{ref}) \tag{10}
$$
$
= -\min_{r} \mathbb{E}_{\mu_E}[f^*(-r)] + \underbrace{\max_{\pi} \mathbb{E}_{\mu_{\pi}}[r] - \beta D_{KL}(\pi \| \pi_{ref})}_{存在闭解}
$
其中，$f^*$为所选f-散度对应的凸共轭函数，$\mu_{\pi}$为待学习策略的状态-动作分布，$\mu_E$为专家的状态-动作分布。


### 证明过程
参考非对抗性分布匹配的推导逻辑[16]，可将训练目标（公式1）重写为含共轭函数的最小-最大（min-max）问题：

已知凸共轭函数的定义为：$f^*(g) = \max_{x \in dom(f)} \{x g - f(x)\}$，且满足对偶性：$f(x) = \max_{g \in dom(f^*)} \{x g - f^*(g)\}$（公式12、13）。

基于此，对原始目标函数进行变形：
$$
\begin{align*}
\min_{\pi} D_f(\mu_{\pi} \| \mu_E) + \beta D_{KL}(\pi \| \pi_{ref}) 
&= \min_{\pi} \int \mu_E f\left( \frac{\mu_{\pi}}{\mu_E} \right) dsda + \beta D_{KL}(\pi \| \pi_{ref}) \\
&= \min_{\pi} \int \mu_E \left( \max_{g: S \times A \to dom(f^*)} \left\{ \frac{\mu_{\pi}}{\mu_E} g - f^*(g) \right\} \right) dsda + \beta D_{KL}(\pi \| \pi_{ref}) \tag{16} \\
&= \min_{\pi} \max_{g: S \times A \to dom(f^*)} -\mathbb{E}_{\mu_E}[f^*(g)] + \mathbb{E}_{\mu_{\pi}}[g] + \beta D_{KL}(\pi \| \pi_{ref}) \tag{17}
\end{align*}
$$
 

将$g$替换为$-r$，可将上述最小-最大形式重写为：
$
-\max_{\pi} \min_{r: S \times A \to -dom(f^*)} \mathbb{E}_{\mu_E}[f^*(-r)] + \mathbb{E}_{\mu_{\pi}}[r] - \beta D_{KL}(\pi \| \pi_{ref})
$

该目标属于鞍点问题：一方面，共轭函数具有凸性；另一方面，KL散度可分解为“凹函数（熵项）”与“关于策略$\pi$的线性项（交叉熵项）”。因此，可交换最小化与最大化的顺序：
$
-\min_{r: S \times A \to -dom(f^*)} \max_{\pi} \mathbb{E}_{\mu_E}[f^*(-r)] + \mathbb{E}_{\mu_{\pi}}[r] - \beta D_{KL}(\pi \| \pi_{ref})
$

根据已有研究[17]，上述表达式中“最大化部分”（仅针对后一项）存在闭解，因此可进一步简化为：
$
-\min_{r} \mathbb{E}_{\mu_E}[f^*(-r)] + \underbrace{\max_{\pi} \mathbb{E}_{\mu_{\pi}}[r] - \beta D_{KL}(\pi \| \pi_{ref})}_{存在闭解}
$


## B.2 SFT是隐式奖励学习的特例
最终目标可推导为：
$$
\begin{align*}
& -\min_{r} \left[ \mathbb{E}_{\mu_E}[f^*(-r)] + \underbrace{\max_{\pi} \mathbb{E}_{\mu_{\pi}}[r] - \beta D_{KL}(\pi \| \pi_{ref})}_{存在闭解} \right] \\
&= -\min_{r} \mathbb{E}_{\mu_E}[f^*(-r)] + V_{\pi}(s_0)
\end{align*}
$$

在整个训练过程中，奖励始终满足公式（4），且策略$\pi$始终是“最大化部分”的最优解。当选择**总变差距离**作为f-散度时，其对应的共轭函数为$f^*(t) = t$，此时训练目标等价于极大似然估计（MLE）项，推导过程如下：
$$
\begin{align*}
& -\min_{r} \mathbb{E}_{\mu_E}[f^*(-r)] + V_{\pi}(s_0) \tag{23} \\
&= \max_{r} \mathbb{E}_{\mu_E}[-f^*(-r)] - V_{\pi}(s_0) \tag{24} \\
&= \max_{r} \mathbb{E}_{\mu_E}[r] - V_{\pi}(s_0) \tag{25} \\
&= \max_{r} \mathbb{E}_{\mu_E} \left[ \log \frac{\pi(y | x)}{\pi_{ref}(y | x)} + V_{\pi}(s_0) - V_{\pi}(s_t) \right] - V_{\pi}(s_0) \tag{26} \\
&= \max_{\pi} \mathbb{E}_{\mu_E} \left[ \beta \underbrace{\log \pi(y | x)}_{极大似然估计（MLE）} - \underbrace{\log \pi_{ref}(y | x)}_{常数项} - V_{\pi}(s_t) \right] \tag{27}
\end{align*}
$$


## B.3 SFT过程中模型保持内在期望回报估计
### 定理4（内在期望回报）
在SFT过程中，语言模型的对数概率$l_a$对应于所学习隐式奖励的Q函数$Q(s, a)$，具体关系如下：
$$
l_a = Q_{\hat{r}}(s, a) + C(s) = \hat{r}(s, a) + \gamma \mathbb{E}_{s_{t+1} \sim P(\cdot | s, a)} \left[ V_{\hat{r}}(s_{t+1}) \right] \tag{28}
$$
其中，$\hat{r}$为模型满足公式（4）的隐式奖励函数，$\gamma$为折扣因子，$V_{\hat{r}}(s_{t+1})$为下一状态的价值函数；$C(s)$为仅依赖于当前状态的函数。


### 证明过程
语言模型通过对对数概率$l$进行softmax运算得到令牌概率分布：
$$
p(a_i | s) = \frac{e^{l_i / \tau}}{\sum_j e^{l_j / \tau}} \tag{29}
$$
其中，$\tau$为温度参数，训练过程中通常设为1。

如前所述，模型可视为某一隐式奖励函数下的最优策略，该最优策略的概率分布满足：
$$
p(a_i | s) = \frac{e^{Q(s, a_i) / \beta}}{\sum_j e^{Q(s, a_j) / \beta}} \tag{30}
$$

令两式相等，可得：
$$
\frac{e^{l_i / \tau}}{\sum_j e^{l_j / \tau}} = \frac{e^{Q(s, a_i) / \beta}}{\sum_j e^{Q(s, a_j) / \beta}} \tag{31}
$$
$$
e^{l_i / \tau} = e^{Q(s, a_i) / \beta} \cdot \frac{\sum_j e^{l_j / \tau}}{\sum_j e^{Q(s, a_j) / \beta}} \tag{32}
$$
$$
e^{l_i / \tau} = k \cdot e^{Q(s, a_i) / \beta} \tag{33}
$$
$$
l_i = \frac{\tau}{\beta} Q(s, a_i) + C(s)
$$

其中，$\beta$为KL散度系数。上述关系表明，对数概率$l_i$与Q值存在线性映射关系；$C(s)$仅依赖于当前状态（因其包含对所有可能动作的求和项）。本研究假设$C(s)$在不同状态间的数值差异较小，相关实证证据将在实验部分提供。
 

 # 附录C 不同f-散度导出不同损失形式
本节将详细呈现由不同f-散度推导得到的损失函数表达式。

| $D_f(P \| Q)$（f-散度） | 训练目标 |
| ------------------------ | -------- |
| 总变差距离（Total Variation） | $\log \pi_{ref}(y | x) - V_{\pi}(s_t)$ |
| 平方海林格散度（Squared Hellinger）<br>$\int(\sqrt{p(x)}-\sqrt{q(x)})^{2} ~dx$ | $\sqrt{\pi_{ref}(y | x)} \cdot e^{\frac{\Delta V}{2}} - \sqrt{\pi(y | x)}$（其中$\Delta V$为$V_{\pi}(s_0) - V_{\pi}(s_t)$的缩写，下同） |
| 詹森-香农散度（Jensen-Shannon Divergence）<br>$\frac{1}{2} \int p(x) \log \frac{2p(x)}{p(x)+q(x)} + q(x) \log \frac{2q(x)}{p(x)+q(x)} dx - \log 4$ | $\log\left(1 + e^{\log \pi_{ref}(y | x) + \Delta V}\right) - \log\left(1 + e^{\log \pi(y | x)}\right)$ |
| 皮尔逊χ²散度（Pearson χ² Divergence）<br>$\int \frac{(p(x)-q(x))^2}{q(x)} dx$ | $\pi(y | x) - \pi_{ref}(y | x) \cdot e^{\Delta V}$ |

表4：针对不同f-散度，列出其对应的共轭函数$f^*$及推导得到的训练目标。为简化表述，用符号$\Delta V$表示$V_{\pi}(s_0) - V_{\pi}(s_t)$。


# 附录D 多目标学习尝试
鉴于有监督微调（SFT）与直接偏好优化（DPO）均对隐式奖励进行建模，本研究尝试将二者构建为多目标学习任务，而非采用现有的序贯训练模式。我们通过**拉格朗日乘数法**实现这一思路：具体而言，在SFT训练过程中，同时要求DPO损失对应的准确率尽可能低，据此构建如下优化目标：

### 多目标优化目标构建
1. 带约束的优化目标：  
$\min _{\pi} \text{SFT损失}, \quad \text{s.t.} \quad (\text{（优选样本-拒绝样本）准确率} - \text{目标准确率}) < \delta$  
其中，$\delta$为超参数，用于控制准确率与目标值的偏差范围。

2. 拉格朗日转化目标：  
为消除约束条件，引入拉格朗日参数$\lambda$，将带约束目标转化为无约束目标：  
$$\min _{\pi} \text{SFT损失} + \lambda \cdot \text{DPO损失} \tag{36}$$

其中，参数$\lambda$的调整规则参考近端策略优化（PPO）：当“实际准确率 - 目标准确率 < $\delta$”时，将$\lambda$调整为$\frac{\lambda}{2}$；否则将$\lambda$设为$2 \cdot \lambda$。


### 实验设置与结果
我们直接在UltraFeedback数据集上训练模型，并记录了一次训练过程中的关键训练指标（如图4所示）。此外，我们还尝试了多种超参数组合（包括$\lambda$的不同增长与衰减系数），但所有配置均导致下游任务性能不佳。

#### 训练指标可视化
图4展示了多目标学习过程中的训练指标变化（横轴为训练步数“train/global_step”，纵轴为指标数值“Value”）：
- 左上：$train/pref\_loss$（偏好损失），随训练步数增加整体呈下降趋势；
- 右上：$train/gpt\_loss\_mean$（GPT损失均值），训练前期下降明显，后期趋于平稳；
- 左下：$train/gpt\_loss\_coff$（GPT损失系数）与$train/dpo\_loss\_coff$（DPO损失系数），二者均随训练波动但整体保持在0-1区间；
- 右下：$train/acc\_mean$（平均准确率），训练过程中无明显上升趋势，始终处于较低水平。

#### 交替训练实验验证
为进一步探究SFT与DPO的目标冲突，我们设计了**交替训练实验**：将SFT与DPO的训练数据各分为4个片段，按“SFT1→DPO1→SFT2→DPO2→SFT3→DPO3→SFT4→DPO4”的顺序交替训练，结果如表5所示。


表5：SFT与DPO交替训练的性能结果（以AlpacaEval 2的胜率与语言一致性胜率为指标）。

从表5可见，**每次DPO训练后，后续的SFT训练都会导致性能显著下降**——例如DPO 3后胜率达12.94%，但SFT 4后胜率降至8.44%。这一现象表明，在本研究的实验设置下，SFT与DPO的优化目标存在内在冲突。


### 失败原因与未来展望
本研究的多目标学习尝试失败，与[14]的理论结论一致：随着SFT损失系数的增加，模型倾向于将概率质量分配到“负区域”（即不符合偏好的输出），从而削弱DPO的“挤压效应”（squeezing effect）——该效应是DPO提升偏好对齐的关键机制。

但这一失败并不意味着多目标学习思路本身不可行：我们尚未在其他任务领域验证其有效性，这一探索留待未来研究。我们推测，在“专家轨迹与SFT假设（训练数据代表最优行为）更契合”的任务中（如数学推理、代码生成等），SFT与DPO的目标冲突可能减弱，多目标学习或能取得更优结果。